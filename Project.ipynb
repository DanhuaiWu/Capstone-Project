{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras cython h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project: \n",
    "  ## Flowers Classification using Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Conv2D, MaxPool2D, Dropout, Activation, Dense, Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.activations import relu, softmax\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "import os\n",
    "# import urllib.request\n",
    "# from urllib.request import Request, urlopen\n",
    "# from urllib.error import URLError\n",
    "# import socket  \n",
    "# socket.setdefaulttimeout(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "\n",
    "### Scrap images from http://www.image-net.org \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# category={'Sunflower':'http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=n11978961',\n",
    "#           'Peony':'http://image-net.org/api/text/imagenet.synset.geturls?wnid=n11719286',\n",
    "#           'Nigella':'http://image-net.org/api/text/imagenet.synset.geturls?wnid=n11736851',\n",
    "#           'Spathiphyllum':'http://image-net.org/api/text/imagenet.synset.geturls?wnid=n11792341',\n",
    "#           'Ragged_robin':'http://image-net.org/api/text/imagenet.synset.geturls?wnid=n11811706',\n",
    "#           'Soapwort':'http://image-net.org/api/text/imagenet.synset.geturls?wnid=n11814584',\n",
    "#           'Ice_plant':'http://image-net.org/api/text/imagenet.synset.geturls?wnid=n11821184',\n",
    "#           'Spring_beauty':'http://image-net.org/api/text/imagenet.synset.geturls?wnid=n11859472',\n",
    "#           'African_daisy':'http://image-net.org/api/text/imagenet.synset.geturls?wnid=n11925303',\n",
    "#           'Cornflower':'http://image-net.org/api/text/imagenet.synset.geturls?wnid=n11947802'\n",
    "#           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import os\n",
    "# import urllib.request\n",
    "# from urllib.request import Request, urlopen\n",
    "# from urllib.error import URLError\n",
    "# import socket  \n",
    "# socket.setdefaulttimeout(1)\n",
    "\n",
    "\n",
    "\n",
    "# def get_urls(urls_links):\n",
    "#     url_list=urllib.request.urlopen(urls_links).read().decode().split('\\r\\n')\n",
    "#     return url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def download_images(urls_link,category_name):\n",
    "#     if not os.path.exists(category_name):\n",
    "#         os.makedirs(category_name)\n",
    "#     count=1\n",
    "#     url_list=get_urls(urls_link)\n",
    "#     for url in url_list:\n",
    "#         try:\n",
    "#             path_name=str(category_name)+'/'+str(count)+'.'+str(category_name)+'.jpg'\n",
    "#             urllib.request.urlretrieve(url,path_name)\n",
    "             \n",
    "#             img=cv2.imread(path_name)\n",
    "#             resized_image=cv2.resize(img,(100,100))\n",
    "#             cv2.imwrite(path_name,resized_image)\n",
    "#         except Exception as e:\n",
    "#             print(str(count)+str(e))\n",
    "#         count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for cate in category:\n",
    "#     download_images(category['cate'],cate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_image_dir='all_flower_images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def label_img(img):\n",
    "    img_label = img.split('.')[-2]\n",
    "    return img_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def jpg_image_to_array(path):\n",
    "    \"\"\"\n",
    "  Loads JPEG image into 3D Numpy array of shape \n",
    "  (width, height, channels)\n",
    "  \"\"\"\n",
    "    img=Image.open(path)\n",
    "    img_arr=np.asarray(img).reshape((img.size[1], img.size[0],3))\n",
    "    return img_arr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_image_list(image_dir):\n",
    "    img_dataset=[]\n",
    "#     label_dataset=[]\n",
    "    for img in os.listdir(image_dir):\n",
    "        try: \n",
    "            img_path=str(image_dir)+'/'+str(img)\n",
    "            image_array=jpg_image_to_array(img_path)\n",
    "            label=label_img(img)\n",
    "            img_dataset.append([image_array])  \n",
    "#             label_dataset.append([label])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "#     np.save('image_dataset.npy',img_dataset)\n",
    "#     return img_dataset\n",
    "\n",
    "    img_list_arr=np.array(img_dataset)\n",
    "    len_img_list=img_list_arr.shape[0]\n",
    "    img_list_arr_reshape=img_list_arr.reshape(len_img_list,100,100,3)\n",
    "\n",
    "\n",
    "    return img_list_arr_reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot reshape array of size 7000 into shape (70,100,3)\n",
      "cannot reshape array of size 90000 into shape (300,300,3)\n",
      "cannot reshape array of size 36936 into shape (216,171,3)\n",
      "cannot identify image file 'all_flower_images/370.Ragged_robin.jpg'\n",
      "cannot identify image file 'all_flower_images/725.Spring_beauty.jpg'\n",
      "cannot reshape array of size 13440 into shape (120,112,3)\n",
      "cannot identify image file 'all_flower_images/438.Spring_beauty.jpg'\n",
      "cannot reshape array of size 9216 into shape (96,96,3)\n",
      "cannot reshape array of size 2700 into shape (60,45,3)\n",
      "cannot reshape array of size 7000 into shape (70,100,3)\n",
      "cannot reshape array of size 12100 into shape (110,110,3)\n",
      "cannot identify image file 'all_flower_images/.DS_Store'\n",
      "cannot identify image file 'all_flower_images/472.Sunflower.jpg'\n",
      "cannot reshape array of size 16320 into shape (120,136,3)\n"
     ]
    }
   ],
   "source": [
    "image_list_arr=create_image_list(all_image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11755, 100, 100, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_list_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_label_list(image_dir):\n",
    "#     img_dataset=[]\n",
    "    label_dataset=[]\n",
    "    for img in os.listdir(image_dir):\n",
    "        try: \n",
    "            img_path=str(image_dir)+'/'+str(img)\n",
    "            image_array=jpg_image_to_array(img_path)\n",
    "            label=label_img(img)\n",
    "#             img_dataset.append([image_array])  \n",
    "            label_dataset.append([label])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "#     np.save('image_dataset.npy',img_dataset)\n",
    "#     return img_dataset\n",
    "\n",
    "    labels_list_arr=np.array(label_dataset)\n",
    "\n",
    "    return labels_list_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot reshape array of size 7000 into shape (70,100,3)\n",
      "cannot reshape array of size 90000 into shape (300,300,3)\n",
      "cannot reshape array of size 36936 into shape (216,171,3)\n",
      "cannot identify image file 'all_flower_images/370.Ragged_robin.jpg'\n",
      "cannot identify image file 'all_flower_images/725.Spring_beauty.jpg'\n",
      "cannot reshape array of size 13440 into shape (120,112,3)\n",
      "cannot identify image file 'all_flower_images/438.Spring_beauty.jpg'\n",
      "cannot reshape array of size 9216 into shape (96,96,3)\n",
      "cannot reshape array of size 2700 into shape (60,45,3)\n",
      "cannot reshape array of size 7000 into shape (70,100,3)\n",
      "cannot reshape array of size 12100 into shape (110,110,3)\n",
      "cannot identify image file 'all_flower_images/.DS_Store'\n",
      "cannot identify image file 'all_flower_images/472.Sunflower.jpg'\n",
      "cannot reshape array of size 16320 into shape (120,136,3)\n"
     ]
    }
   ],
   "source": [
    "label_list_arr=create_label_list(all_image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11755, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_test_split(X, y , percentage_of_train):\n",
    "    a = X\n",
    "    b = y\n",
    "    c = list(zip(a,b))\n",
    "    random.shuffle(c)\n",
    "    a1, b1 = zip(*c)\n",
    "    a2=np.array(a1)\n",
    "    b2=np.array(b1)\n",
    "    a2=a2.reshape(a.shape[0], 100, 100, 3)\n",
    "    \n",
    "    percentage=int(a.shape[0]*percentage_of_train)\n",
    "    \n",
    "    XX_train=a2[:percentage]\n",
    "    XX_test=a2[percentage:]\n",
    "    yy_train=b2[:percentage]\n",
    "    yy_test=b2[percentage:]\n",
    "    \n",
    "    return XX_train, XX_test, yy_train, yy_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test=train_test_split(image_list_arr,label_list_arr,0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9404, 1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2351, 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# X_train_rescale=X_train/255\n",
    "X_test_rescale=X_test/255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 157.,  183.,  172.],\n",
       "         [ 153.,  178.,  175.],\n",
       "         [ 141.,  162.,  155.],\n",
       "         ..., \n",
       "         [ 132.,  155.,   87.],\n",
       "         [ 138.,  161.,   91.],\n",
       "         [ 152.,  178.,  105.]],\n",
       "\n",
       "        [[ 164.,  188.,  175.],\n",
       "         [ 159.,  182.,  174.],\n",
       "         [ 157.,  183.,  172.],\n",
       "         ..., \n",
       "         [ 121.,  143.,   71.],\n",
       "         [ 131.,  153.,   80.],\n",
       "         [ 161.,  185.,  107.]],\n",
       "\n",
       "        [[ 178.,  198.,  189.],\n",
       "         [ 165.,  188.,  178.],\n",
       "         [ 164.,  196.,  175.],\n",
       "         ..., \n",
       "         [ 119.,  144.,   76.],\n",
       "         [ 131.,  157.,   83.],\n",
       "         [ 125.,  154.,   74.]],\n",
       "\n",
       "        ..., \n",
       "        [[ 112.,  164.,   89.],\n",
       "         [ 119.,  170.,   95.],\n",
       "         [ 125.,  172.,   94.],\n",
       "         ..., \n",
       "         [ 157.,  199.,  136.],\n",
       "         [ 144.,  186.,  123.],\n",
       "         [ 116.,  158.,   95.]],\n",
       "\n",
       "        [[ 112.,  165.,   85.],\n",
       "         [ 121.,  172.,   93.],\n",
       "         [ 110.,  157.,   79.],\n",
       "         ..., \n",
       "         [ 120.,  159.,  104.],\n",
       "         [ 127.,  166.,  111.],\n",
       "         [ 148.,  187.,  132.]],\n",
       "\n",
       "        [[  94.,  148.,   64.],\n",
       "         [ 111.,  163.,   81.],\n",
       "         [  97.,  144.,   64.],\n",
       "         ..., \n",
       "         [ 135.,  172.,  120.],\n",
       "         [ 138.,  175.,  124.],\n",
       "         [ 160.,  196.,  148.]]],\n",
       "\n",
       "\n",
       "       [[[ 206.,  148.,  172.],\n",
       "         [ 184.,  126.,  150.],\n",
       "         [ 167.,  111.,  136.],\n",
       "         ..., \n",
       "         [ 124.,  106.,   96.],\n",
       "         [ 124.,  104.,   95.],\n",
       "         [ 122.,   99.,   93.]],\n",
       "\n",
       "        [[ 203.,  145.,  167.],\n",
       "         [ 181.,  126.,  149.],\n",
       "         [ 151.,   97.,  120.],\n",
       "         ..., \n",
       "         [ 138.,  118.,  109.],\n",
       "         [ 126.,  106.,   99.],\n",
       "         [ 103.,   79.,   75.]],\n",
       "\n",
       "        [[ 197.,  144.,  162.],\n",
       "         [ 184.,  131.,  149.],\n",
       "         [ 152.,  101.,  120.],\n",
       "         ..., \n",
       "         [ 118.,   98.,   91.],\n",
       "         [ 118.,   97.,   92.],\n",
       "         [ 100.,   76.,   74.]],\n",
       "\n",
       "        ..., \n",
       "        [[ 137.,   66.,   70.],\n",
       "         [ 136.,   80.,   81.],\n",
       "         [ 150.,  111.,  112.],\n",
       "         ..., \n",
       "         [  71.,   62.,   55.],\n",
       "         [  63.,   54.,   47.],\n",
       "         [  98.,   88.,   79.]],\n",
       "\n",
       "        [[ 158.,   94.,   94.],\n",
       "         [ 137.,   87.,   86.],\n",
       "         [ 124.,   90.,   88.],\n",
       "         ..., \n",
       "         [  91.,   82.,   75.],\n",
       "         [  74.,   63.,   57.],\n",
       "         [ 123.,  110.,  102.]],\n",
       "\n",
       "        [[ 169.,  111.,  109.],\n",
       "         [ 125.,   82.,   76.],\n",
       "         [ 104.,   79.,   74.],\n",
       "         ..., \n",
       "         [ 115.,  104.,  100.],\n",
       "         [  77.,   64.,   58.],\n",
       "         [ 105.,   90.,   83.]]],\n",
       "\n",
       "\n",
       "       [[[  70.,  120.,  155.],\n",
       "         [  71.,  122.,  165.],\n",
       "         [  69.,  121.,  168.],\n",
       "         ..., \n",
       "         [  81.,  101.,   16.],\n",
       "         [  84.,  103.,   21.],\n",
       "         [  87.,  106.,   24.]],\n",
       "\n",
       "        [[  64.,  114.,  147.],\n",
       "         [  65.,  113.,  151.],\n",
       "         [  70.,  120.,  157.],\n",
       "         ..., \n",
       "         [  79.,   99.,   14.],\n",
       "         [  83.,  103.,   18.],\n",
       "         [  86.,  106.,   21.]],\n",
       "\n",
       "        [[  56.,  101.,  124.],\n",
       "         [  52.,   95.,  114.],\n",
       "         [  66.,  105.,  122.],\n",
       "         ..., \n",
       "         [  77.,   97.,   10.],\n",
       "         [  80.,  100.,   13.],\n",
       "         [  81.,  104.,   16.]],\n",
       "\n",
       "        ..., \n",
       "        [[  50.,   65.,   24.],\n",
       "         [  29.,   47.,    7.],\n",
       "         [  19.,   43.,    9.],\n",
       "         ..., \n",
       "         [  17.,   35.,    0.],\n",
       "         [  37.,   57.,    6.],\n",
       "         [  21.,   46.,    0.]],\n",
       "\n",
       "        [[  22.,   31.,   28.],\n",
       "         [  18.,   34.,    7.],\n",
       "         [  23.,   49.,    2.],\n",
       "         ..., \n",
       "         [  21.,   38.,    2.],\n",
       "         [  74.,   94.,   41.],\n",
       "         [ 103.,  129.,   56.]],\n",
       "\n",
       "        [[  23.,   30.,   40.],\n",
       "         [  21.,   38.,    6.],\n",
       "         [  71.,  101.,   31.],\n",
       "         ..., \n",
       "         [  28.,   46.,    6.],\n",
       "         [  37.,   58.,    2.],\n",
       "         [  41.,   67.,    0.]]],\n",
       "\n",
       "\n",
       "       ..., \n",
       "       [[[ 120.,   94.,   67.],\n",
       "         [ 116.,   88.,   66.],\n",
       "         [ 104.,   72.,   51.],\n",
       "         ..., \n",
       "         [ 110.,  162.,  219.],\n",
       "         [ 108.,  160.,  217.],\n",
       "         [ 107.,  159.,  216.]],\n",
       "\n",
       "        [[ 110.,   92.,   68.],\n",
       "         [ 104.,   85.,   68.],\n",
       "         [  89.,   70.,   56.],\n",
       "         ..., \n",
       "         [ 110.,  162.,  219.],\n",
       "         [ 109.,  161.,  218.],\n",
       "         [ 108.,  160.,  217.]],\n",
       "\n",
       "        [[ 105.,   92.,   75.],\n",
       "         [ 102.,   92.,   82.],\n",
       "         [  86.,   79.,   73.],\n",
       "         ..., \n",
       "         [ 112.,  164.,  221.],\n",
       "         [ 111.,  163.,  220.],\n",
       "         [ 110.,  162.,  219.]],\n",
       "\n",
       "        ..., \n",
       "        [[ 110.,  134.,   10.],\n",
       "         [ 119.,  145.,   18.],\n",
       "         [ 109.,  133.,   21.],\n",
       "         ..., \n",
       "         [  44.,   58.,   33.],\n",
       "         [ 161.,  169.,  146.],\n",
       "         [ 182.,  184.,  162.]],\n",
       "\n",
       "        [[ 108.,  127.,   37.],\n",
       "         [ 102.,  123.,   32.],\n",
       "         [ 144.,  161.,   83.],\n",
       "         ..., \n",
       "         [  41.,   48.,   30.],\n",
       "         [  41.,   42.,   24.],\n",
       "         [ 237.,  234.,  217.]],\n",
       "\n",
       "        [[ 171.,  184.,  131.],\n",
       "         [ 164.,  178.,  127.],\n",
       "         [ 161.,  172.,  129.],\n",
       "         ..., \n",
       "         [  37.,   39.,   25.],\n",
       "         [  49.,   45.,   33.],\n",
       "         [ 106.,   96.,   86.]]],\n",
       "\n",
       "\n",
       "       [[[ 185.,  126.,   82.],\n",
       "         [ 186.,  129.,   84.],\n",
       "         [ 157.,  105.,   58.],\n",
       "         ..., \n",
       "         [  45.,   60.,   53.],\n",
       "         [  49.,   64.,   57.],\n",
       "         [  11.,   26.,   19.]],\n",
       "\n",
       "        [[ 179.,  125.,   91.],\n",
       "         [ 149.,   96.,   62.],\n",
       "         [ 186.,  136.,   99.],\n",
       "         ..., \n",
       "         [  46.,   58.,   54.],\n",
       "         [  28.,   40.,   36.],\n",
       "         [  17.,   29.,   25.]],\n",
       "\n",
       "        [[ 143.,   94.,   80.],\n",
       "         [ 114.,   65.,   50.],\n",
       "         [ 114.,   69.,   50.],\n",
       "         ..., \n",
       "         [  39.,   49.,   50.],\n",
       "         [  10.,   20.,   21.],\n",
       "         [  26.,   36.,   37.]],\n",
       "\n",
       "        ..., \n",
       "        [[ 111.,   99.,  111.],\n",
       "         [  71.,   71.,   71.],\n",
       "         [  30.,   42.,   30.],\n",
       "         ..., \n",
       "         [  97.,   83.,   72.],\n",
       "         [  96.,   82.,   71.],\n",
       "         [  97.,   83.,   72.]],\n",
       "\n",
       "        [[  79.,   67.,   79.],\n",
       "         [  27.,   27.,   27.],\n",
       "         [  43.,   55.,   43.],\n",
       "         ..., \n",
       "         [ 102.,   88.,   77.],\n",
       "         [ 100.,   86.,   75.],\n",
       "         [ 101.,   87.,   76.]],\n",
       "\n",
       "        [[  26.,   14.,   26.],\n",
       "         [  16.,   16.,   16.],\n",
       "         [  46.,   58.,   46.],\n",
       "         ..., \n",
       "         [ 103.,   89.,   78.],\n",
       "         [ 101.,   87.,   76.],\n",
       "         [ 100.,   86.,   75.]]],\n",
       "\n",
       "\n",
       "       [[[ 184.,  169.,  150.],\n",
       "         [ 242.,  227.,  208.],\n",
       "         [ 255.,  247.,  229.],\n",
       "         ..., \n",
       "         [ 166.,  158.,  119.],\n",
       "         [ 159.,  149.,  113.],\n",
       "         [ 122.,  105.,   75.]],\n",
       "\n",
       "        [[ 203.,  186.,  168.],\n",
       "         [ 250.,  233.,  215.],\n",
       "         [ 255.,  239.,  222.],\n",
       "         ..., \n",
       "         [ 159.,  154.,  114.],\n",
       "         [ 145.,  135.,   99.],\n",
       "         [ 113.,   98.,   67.]],\n",
       "\n",
       "        [[ 188.,  170.,  150.],\n",
       "         [ 255.,  243.,  224.],\n",
       "         [ 255.,  249.,  230.],\n",
       "         ..., \n",
       "         [ 148.,  143.,  105.],\n",
       "         [ 131.,  121.,   86.],\n",
       "         [ 116.,  101.,   70.]],\n",
       "\n",
       "        ..., \n",
       "        [[  42.,   54.,   18.],\n",
       "         [  88.,  101.,   58.],\n",
       "         [ 127.,  141.,   92.],\n",
       "         ..., \n",
       "         [  56.,   27.,   11.],\n",
       "         [ 161.,  126.,  106.],\n",
       "         [ 155.,  113.,   89.]],\n",
       "\n",
       "        [[  74.,   86.,   46.],\n",
       "         [ 104.,  117.,   73.],\n",
       "         [ 125.,  139.,   86.],\n",
       "         ..., \n",
       "         [  56.,   27.,   11.],\n",
       "         [ 161.,  126.,  106.],\n",
       "         [ 155.,  113.,   89.]],\n",
       "\n",
       "        [[  76.,   89.,   43.],\n",
       "         [  96.,  110.,   59.],\n",
       "         [ 107.,  122.,   65.],\n",
       "         ..., \n",
       "         [  56.,   27.,   11.],\n",
       "         [ 161.,  126.,  106.],\n",
       "         [ 155.,  113.,   89.]]]], dtype=float32)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.6156863 ,  0.71764708,  0.67450982],\n",
       "         [ 0.60000002,  0.69803923,  0.68627453],\n",
       "         [ 0.5529412 ,  0.63529414,  0.60784316],\n",
       "         ..., \n",
       "         [ 0.51764709,  0.60784316,  0.34117648],\n",
       "         [ 0.5411765 ,  0.63137257,  0.35686275],\n",
       "         [ 0.59607846,  0.69803923,  0.41176471]],\n",
       "\n",
       "        [[ 0.64313728,  0.73725492,  0.68627453],\n",
       "         [ 0.62352943,  0.71372551,  0.68235296],\n",
       "         [ 0.6156863 ,  0.71764708,  0.67450982],\n",
       "         ..., \n",
       "         [ 0.47450981,  0.56078434,  0.27843139],\n",
       "         [ 0.51372552,  0.60000002,  0.3137255 ],\n",
       "         [ 0.63137257,  0.72549021,  0.41960785]],\n",
       "\n",
       "        [[ 0.69803923,  0.7764706 ,  0.74117649],\n",
       "         [ 0.64705884,  0.73725492,  0.69803923],\n",
       "         [ 0.64313728,  0.76862746,  0.68627453],\n",
       "         ..., \n",
       "         [ 0.46666667,  0.56470591,  0.29803923],\n",
       "         [ 0.51372552,  0.6156863 ,  0.32549021],\n",
       "         [ 0.49019608,  0.60392159,  0.29019609]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.43921569,  0.64313728,  0.34901962],\n",
       "         [ 0.46666667,  0.66666669,  0.37254903],\n",
       "         [ 0.49019608,  0.67450982,  0.36862746],\n",
       "         ..., \n",
       "         [ 0.6156863 ,  0.78039217,  0.53333336],\n",
       "         [ 0.56470591,  0.72941178,  0.48235294],\n",
       "         [ 0.45490196,  0.61960787,  0.37254903]],\n",
       "\n",
       "        [[ 0.43921569,  0.64705884,  0.33333334],\n",
       "         [ 0.47450981,  0.67450982,  0.36470589],\n",
       "         [ 0.43137255,  0.6156863 ,  0.30980393],\n",
       "         ..., \n",
       "         [ 0.47058824,  0.62352943,  0.40784314],\n",
       "         [ 0.49803922,  0.65098041,  0.43529412],\n",
       "         [ 0.58039218,  0.73333335,  0.51764709]],\n",
       "\n",
       "        [[ 0.36862746,  0.58039218,  0.25098041],\n",
       "         [ 0.43529412,  0.63921571,  0.31764707],\n",
       "         [ 0.38039216,  0.56470591,  0.25098041],\n",
       "         ..., \n",
       "         [ 0.52941179,  0.67450982,  0.47058824],\n",
       "         [ 0.5411765 ,  0.68627453,  0.48627451],\n",
       "         [ 0.627451  ,  0.76862746,  0.58039218]]],\n",
       "\n",
       "\n",
       "       [[[ 0.80784315,  0.58039218,  0.67450982],\n",
       "         [ 0.72156864,  0.49411765,  0.58823532],\n",
       "         [ 0.65490198,  0.43529412,  0.53333336],\n",
       "         ..., \n",
       "         [ 0.48627451,  0.41568628,  0.3764706 ],\n",
       "         [ 0.48627451,  0.40784314,  0.37254903],\n",
       "         [ 0.47843137,  0.3882353 ,  0.36470589]],\n",
       "\n",
       "        [[ 0.79607844,  0.56862748,  0.65490198],\n",
       "         [ 0.70980394,  0.49411765,  0.58431375],\n",
       "         [ 0.59215689,  0.38039216,  0.47058824],\n",
       "         ..., \n",
       "         [ 0.5411765 ,  0.4627451 ,  0.42745098],\n",
       "         [ 0.49411765,  0.41568628,  0.3882353 ],\n",
       "         [ 0.40392157,  0.30980393,  0.29411766]],\n",
       "\n",
       "        [[ 0.77254903,  0.56470591,  0.63529414],\n",
       "         [ 0.72156864,  0.51372552,  0.58431375],\n",
       "         [ 0.59607846,  0.39607844,  0.47058824],\n",
       "         ..., \n",
       "         [ 0.4627451 ,  0.38431373,  0.35686275],\n",
       "         [ 0.4627451 ,  0.38039216,  0.36078432],\n",
       "         [ 0.39215687,  0.29803923,  0.29019609]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.53725493,  0.25882354,  0.27450982],\n",
       "         [ 0.53333336,  0.3137255 ,  0.31764707],\n",
       "         [ 0.58823532,  0.43529412,  0.43921569],\n",
       "         ..., \n",
       "         [ 0.27843139,  0.24313726,  0.21568628],\n",
       "         [ 0.24705882,  0.21176471,  0.18431373],\n",
       "         [ 0.38431373,  0.34509805,  0.30980393]],\n",
       "\n",
       "        [[ 0.61960787,  0.36862746,  0.36862746],\n",
       "         [ 0.53725493,  0.34117648,  0.33725491],\n",
       "         [ 0.48627451,  0.35294119,  0.34509805],\n",
       "         ..., \n",
       "         [ 0.35686275,  0.32156864,  0.29411766],\n",
       "         [ 0.29019609,  0.24705882,  0.22352941],\n",
       "         [ 0.48235294,  0.43137255,  0.40000001]],\n",
       "\n",
       "        [[ 0.66274512,  0.43529412,  0.42745098],\n",
       "         [ 0.49019608,  0.32156864,  0.29803923],\n",
       "         [ 0.40784314,  0.30980393,  0.29019609],\n",
       "         ..., \n",
       "         [ 0.4509804 ,  0.40784314,  0.39215687],\n",
       "         [ 0.3019608 ,  0.25098041,  0.22745098],\n",
       "         [ 0.41176471,  0.35294119,  0.32549021]]],\n",
       "\n",
       "\n",
       "       [[[ 0.27450982,  0.47058824,  0.60784316],\n",
       "         [ 0.27843139,  0.47843137,  0.64705884],\n",
       "         [ 0.27058825,  0.47450981,  0.65882355],\n",
       "         ..., \n",
       "         [ 0.31764707,  0.39607844,  0.0627451 ],\n",
       "         [ 0.32941177,  0.40392157,  0.08235294],\n",
       "         [ 0.34117648,  0.41568628,  0.09411765]],\n",
       "\n",
       "        [[ 0.25098041,  0.44705883,  0.57647061],\n",
       "         [ 0.25490198,  0.44313726,  0.59215689],\n",
       "         [ 0.27450982,  0.47058824,  0.6156863 ],\n",
       "         ..., \n",
       "         [ 0.30980393,  0.3882353 ,  0.05490196],\n",
       "         [ 0.32549021,  0.40392157,  0.07058824],\n",
       "         [ 0.33725491,  0.41568628,  0.08235294]],\n",
       "\n",
       "        [[ 0.21960784,  0.39607844,  0.48627451],\n",
       "         [ 0.20392157,  0.37254903,  0.44705883],\n",
       "         [ 0.25882354,  0.41176471,  0.47843137],\n",
       "         ..., \n",
       "         [ 0.3019608 ,  0.38039216,  0.03921569],\n",
       "         [ 0.3137255 ,  0.39215687,  0.05098039],\n",
       "         [ 0.31764707,  0.40784314,  0.0627451 ]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.19607843,  0.25490198,  0.09411765],\n",
       "         [ 0.11372549,  0.18431373,  0.02745098],\n",
       "         [ 0.07450981,  0.16862746,  0.03529412],\n",
       "         ..., \n",
       "         [ 0.06666667,  0.13725491,  0.        ],\n",
       "         [ 0.14509805,  0.22352941,  0.02352941],\n",
       "         [ 0.08235294,  0.18039216,  0.        ]],\n",
       "\n",
       "        [[ 0.08627451,  0.12156863,  0.10980392],\n",
       "         [ 0.07058824,  0.13333334,  0.02745098],\n",
       "         [ 0.09019608,  0.19215687,  0.00784314],\n",
       "         ..., \n",
       "         [ 0.08235294,  0.14901961,  0.00784314],\n",
       "         [ 0.29019609,  0.36862746,  0.16078432],\n",
       "         [ 0.40392157,  0.50588238,  0.21960784]],\n",
       "\n",
       "        [[ 0.09019608,  0.11764706,  0.15686275],\n",
       "         [ 0.08235294,  0.14901961,  0.02352941],\n",
       "         [ 0.27843139,  0.39607844,  0.12156863],\n",
       "         ..., \n",
       "         [ 0.10980392,  0.18039216,  0.02352941],\n",
       "         [ 0.14509805,  0.22745098,  0.00784314],\n",
       "         [ 0.16078432,  0.26274511,  0.        ]]],\n",
       "\n",
       "\n",
       "       ..., \n",
       "       [[[ 0.47058824,  0.36862746,  0.26274511],\n",
       "         [ 0.45490196,  0.34509805,  0.25882354],\n",
       "         [ 0.40784314,  0.28235295,  0.2       ],\n",
       "         ..., \n",
       "         [ 0.43137255,  0.63529414,  0.85882354],\n",
       "         [ 0.42352942,  0.627451  ,  0.8509804 ],\n",
       "         [ 0.41960785,  0.62352943,  0.84705883]],\n",
       "\n",
       "        [[ 0.43137255,  0.36078432,  0.26666668],\n",
       "         [ 0.40784314,  0.33333334,  0.26666668],\n",
       "         [ 0.34901962,  0.27450982,  0.21960784],\n",
       "         ..., \n",
       "         [ 0.43137255,  0.63529414,  0.85882354],\n",
       "         [ 0.42745098,  0.63137257,  0.85490197],\n",
       "         [ 0.42352942,  0.627451  ,  0.8509804 ]],\n",
       "\n",
       "        [[ 0.41176471,  0.36078432,  0.29411766],\n",
       "         [ 0.40000001,  0.36078432,  0.32156864],\n",
       "         [ 0.33725491,  0.30980393,  0.28627452],\n",
       "         ..., \n",
       "         [ 0.43921569,  0.64313728,  0.86666667],\n",
       "         [ 0.43529412,  0.63921571,  0.86274511],\n",
       "         [ 0.43137255,  0.63529414,  0.85882354]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.43137255,  0.52549022,  0.03921569],\n",
       "         [ 0.46666667,  0.56862748,  0.07058824],\n",
       "         [ 0.42745098,  0.52156866,  0.08235294],\n",
       "         ..., \n",
       "         [ 0.17254902,  0.22745098,  0.12941177],\n",
       "         [ 0.63137257,  0.66274512,  0.57254905],\n",
       "         [ 0.71372551,  0.72156864,  0.63529414]],\n",
       "\n",
       "        [[ 0.42352942,  0.49803922,  0.14509805],\n",
       "         [ 0.40000001,  0.48235294,  0.1254902 ],\n",
       "         [ 0.56470591,  0.63137257,  0.32549021],\n",
       "         ..., \n",
       "         [ 0.16078432,  0.1882353 ,  0.11764706],\n",
       "         [ 0.16078432,  0.16470589,  0.09411765],\n",
       "         [ 0.92941177,  0.91764706,  0.8509804 ]],\n",
       "\n",
       "        [[ 0.67058825,  0.72156864,  0.51372552],\n",
       "         [ 0.64313728,  0.69803923,  0.49803922],\n",
       "         [ 0.63137257,  0.67450982,  0.50588238],\n",
       "         ..., \n",
       "         [ 0.14509805,  0.15294118,  0.09803922],\n",
       "         [ 0.19215687,  0.17647059,  0.12941177],\n",
       "         [ 0.41568628,  0.3764706 ,  0.33725491]]],\n",
       "\n",
       "\n",
       "       [[[ 0.72549021,  0.49411765,  0.32156864],\n",
       "         [ 0.72941178,  0.50588238,  0.32941177],\n",
       "         [ 0.6156863 ,  0.41176471,  0.22745098],\n",
       "         ..., \n",
       "         [ 0.17647059,  0.23529412,  0.20784314],\n",
       "         [ 0.19215687,  0.25098041,  0.22352941],\n",
       "         [ 0.04313726,  0.10196079,  0.07450981]],\n",
       "\n",
       "        [[ 0.7019608 ,  0.49019608,  0.35686275],\n",
       "         [ 0.58431375,  0.3764706 ,  0.24313726],\n",
       "         [ 0.72941178,  0.53333336,  0.3882353 ],\n",
       "         ..., \n",
       "         [ 0.18039216,  0.22745098,  0.21176471],\n",
       "         [ 0.10980392,  0.15686275,  0.14117648],\n",
       "         [ 0.06666667,  0.11372549,  0.09803922]],\n",
       "\n",
       "        [[ 0.56078434,  0.36862746,  0.3137255 ],\n",
       "         [ 0.44705883,  0.25490198,  0.19607843],\n",
       "         [ 0.44705883,  0.27058825,  0.19607843],\n",
       "         ..., \n",
       "         [ 0.15294118,  0.19215687,  0.19607843],\n",
       "         [ 0.03921569,  0.07843138,  0.08235294],\n",
       "         [ 0.10196079,  0.14117648,  0.14509805]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.43529412,  0.3882353 ,  0.43529412],\n",
       "         [ 0.27843139,  0.27843139,  0.27843139],\n",
       "         [ 0.11764706,  0.16470589,  0.11764706],\n",
       "         ..., \n",
       "         [ 0.38039216,  0.32549021,  0.28235295],\n",
       "         [ 0.3764706 ,  0.32156864,  0.27843139],\n",
       "         [ 0.38039216,  0.32549021,  0.28235295]],\n",
       "\n",
       "        [[ 0.30980393,  0.26274511,  0.30980393],\n",
       "         [ 0.10588235,  0.10588235,  0.10588235],\n",
       "         [ 0.16862746,  0.21568628,  0.16862746],\n",
       "         ..., \n",
       "         [ 0.40000001,  0.34509805,  0.3019608 ],\n",
       "         [ 0.39215687,  0.33725491,  0.29411766],\n",
       "         [ 0.39607844,  0.34117648,  0.29803923]],\n",
       "\n",
       "        [[ 0.10196079,  0.05490196,  0.10196079],\n",
       "         [ 0.0627451 ,  0.0627451 ,  0.0627451 ],\n",
       "         [ 0.18039216,  0.22745098,  0.18039216],\n",
       "         ..., \n",
       "         [ 0.40392157,  0.34901962,  0.30588236],\n",
       "         [ 0.39607844,  0.34117648,  0.29803923],\n",
       "         [ 0.39215687,  0.33725491,  0.29411766]]],\n",
       "\n",
       "\n",
       "       [[[ 0.72156864,  0.66274512,  0.58823532],\n",
       "         [ 0.94901961,  0.89019608,  0.81568629],\n",
       "         [ 1.        ,  0.96862745,  0.89803922],\n",
       "         ..., \n",
       "         [ 0.65098041,  0.61960787,  0.46666667],\n",
       "         [ 0.62352943,  0.58431375,  0.44313726],\n",
       "         [ 0.47843137,  0.41176471,  0.29411766]],\n",
       "\n",
       "        [[ 0.79607844,  0.72941178,  0.65882355],\n",
       "         [ 0.98039216,  0.9137255 ,  0.84313726],\n",
       "         [ 1.        ,  0.93725491,  0.87058824],\n",
       "         ..., \n",
       "         [ 0.62352943,  0.60392159,  0.44705883],\n",
       "         [ 0.56862748,  0.52941179,  0.3882353 ],\n",
       "         [ 0.44313726,  0.38431373,  0.26274511]],\n",
       "\n",
       "        [[ 0.73725492,  0.66666669,  0.58823532],\n",
       "         [ 1.        ,  0.95294118,  0.87843138],\n",
       "         [ 1.        ,  0.97647059,  0.90196079],\n",
       "         ..., \n",
       "         [ 0.58039218,  0.56078434,  0.41176471],\n",
       "         [ 0.51372552,  0.47450981,  0.33725491],\n",
       "         [ 0.45490196,  0.39607844,  0.27450982]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.16470589,  0.21176471,  0.07058824],\n",
       "         [ 0.34509805,  0.39607844,  0.22745098],\n",
       "         [ 0.49803922,  0.5529412 ,  0.36078432],\n",
       "         ..., \n",
       "         [ 0.21960784,  0.10588235,  0.04313726],\n",
       "         [ 0.63137257,  0.49411765,  0.41568628],\n",
       "         [ 0.60784316,  0.44313726,  0.34901962]],\n",
       "\n",
       "        [[ 0.29019609,  0.33725491,  0.18039216],\n",
       "         [ 0.40784314,  0.45882353,  0.28627452],\n",
       "         [ 0.49019608,  0.54509807,  0.33725491],\n",
       "         ..., \n",
       "         [ 0.21960784,  0.10588235,  0.04313726],\n",
       "         [ 0.63137257,  0.49411765,  0.41568628],\n",
       "         [ 0.60784316,  0.44313726,  0.34901962]],\n",
       "\n",
       "        [[ 0.29803923,  0.34901962,  0.16862746],\n",
       "         [ 0.3764706 ,  0.43137255,  0.23137255],\n",
       "         [ 0.41960785,  0.47843137,  0.25490198],\n",
       "         ..., \n",
       "         [ 0.21960784,  0.10588235,  0.04313726],\n",
       "         [ 0.63137257,  0.49411765,  0.41568628],\n",
       "         [ 0.60784316,  0.44313726,  0.34901962]]]], dtype=float32)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_rescale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "encoder = LabelBinarizer()\n",
    "y_train_1hot = encoder.fit_transform(y_train)\n",
    "y_test_1hot=encoder.fit_transform(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ..., \n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 0]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_1hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       ..., \n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_1hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# y_train_test11 = keras.utils.to_categorical(y_train_test1, 10)\n",
    "# y_test_test11 = keras.utils.to_categorical(y_test_test1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "sgd = optimizers.SGD(lr=.2, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn1 = Sequential([\n",
    "    Conv2D(30, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=(100,100,3)),\n",
    "#     BatchNormalization(axis=-1),\n",
    "#     Dropout(.25),\n",
    "    MaxPool2D((2,2)),\n",
    "    \n",
    "    Conv2D(70, (3, 3), activation='relu'),\n",
    "    BatchNormalization(axis=-1),\n",
    "    Dropout(.25),\n",
    "    MaxPool2D((2,2)),\n",
    "    \n",
    "#     Conv2D(100, (3, 3), activation='relu'),\n",
    "#     BatchNormalization(axis=-1),\n",
    "#     Dropout(.5),\n",
    "#     MaxPool2D((2,2)),\n",
    "    \n",
    "#     Conv2D(150, (3, 3), activation='relu'),\n",
    "#     BatchNormalization(axis=-1),\n",
    "#     Dropout(.25),\n",
    "#     MaxPool2D((2,2)),\n",
    "    \n",
    "#     Conv2D(100, (3, 3), activation='relu'),\n",
    "#     Dropout(.25),\n",
    "#     MaxPool2D((2,2)),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "#     BatchNormalization(axis=-1),\n",
    "    Dropout(.5),\n",
    "    Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 98, 98, 30)        840       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 49, 49, 30)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 47, 47, 70)        18970     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 47, 47, 70)        280       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 47, 47, 70)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 23, 23, 70)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 37030)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               4739968   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 4,761,348\n",
      "Trainable params: 4,761,208\n",
      "Non-trainable params: 140\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn1.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(lr=0.5),\n",
    "#               optimizer=Adam(lr=0.01),\n",
    "#               optimizer=sgd,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_1hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn1.fit(X_train_rescale, y_train_1hot, validation_split=.2, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy1 = cnn1.evaluate(X_test_rescale, y_test_1hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "pickle.dump(cnn1, open('cnn1.sav', 'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "loaded_model1 = pickle.load(open('cnn1.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "number_of_conv   dropout     num_fillter    Dense   LR   epochs   train_acc   val_acc   test_acc\n",
    "    2               0.25       50 100        128   0.5     5       0.7          0.61       0.57\n",
    "    3               0.25       50 100 200    128   0.5     5       0.67         0.60       0.57 \n",
    "    2               0.5        50 100         64   0.2     5       0.49         0.53       0.58\n",
    "    2               0.5        50 100        128   0.2    20       0.85         0.63       0.63  \n",
    "    \n",
    "    \n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn6 = Sequential([\n",
    "    Conv2D(40, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=(100,100,3)),\n",
    "#     Dropout(.25),\n",
    "    MaxPool2D((2,2)),\n",
    "    \n",
    "    Conv2D(50, (3, 3), activation='relu'),\n",
    "    BatchNormalization(axis=-1),\n",
    "    Dropout(.25),\n",
    "    MaxPool2D((2,2)),\n",
    "    \n",
    "    Conv2D(60, (3, 3), activation='relu'),\n",
    "    BatchNormalization(axis=-1),\n",
    "    Dropout(.5),\n",
    "    MaxPool2D((2,2)),\n",
    "    \n",
    "#     Conv2D(150, (3, 3), activation='relu'),\n",
    "#     BatchNormalization(axis=-1),\n",
    "#     Dropout(.25),\n",
    "#     MaxPool2D((2,2)),\n",
    "    \n",
    "#     Conv2D(100, (3, 3), activation='relu'),\n",
    "#     Dropout(.25),\n",
    "#     MaxPool2D((2,2)),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(32, activation='relu'),\n",
    "    BatchNormalization(axis=-1),\n",
    "    Dropout(.5),\n",
    "    Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_16 (Conv2D)           (None, 98, 98, 40)        1120      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 49, 49, 40)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 47, 47, 50)        18050     \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 47, 47, 50)        200       \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 47, 47, 50)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 23, 23, 50)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 21, 21, 60)        27060     \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 21, 21, 60)        240       \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 21, 21, 60)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 10, 10, 60)        0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 6000)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 32)                192032    \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 239,160\n",
      "Trainable params: 238,876\n",
      "Non-trainable params: 284\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from keras import optimizers\n",
    "# sgd = optimizers.SGD(lr=.2, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn6.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(lr=0.1),\n",
    "#               optimizer=Adam(lr=0.2),\n",
    "#               optimizer=sgd,\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "imgen_train = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=90,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.15,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    ")\n",
    "\n",
    "imgen_test=ImageDataGenerator(rescale=1./255)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_generator = imgen_train.flow(X_train, y_train_1hot, batch_size=64)\n",
    "test_generator = imgen_test.flow(X_test, y_test_1hot, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "146/146 [==============================] - 19s - loss: 2.9247 - acc: 0.1631 - val_loss: 2.7689 - val_acc: 0.1159\n",
      "Epoch 2/100\n",
      "146/146 [==============================] - 16s - loss: 2.5161 - acc: 0.2322 - val_loss: 4.4014 - val_acc: 0.1408\n",
      "Epoch 3/100\n",
      "146/146 [==============================] - 16s - loss: 2.3038 - acc: 0.2716 - val_loss: 4.6802 - val_acc: 0.1727\n",
      "Epoch 4/100\n",
      "146/146 [==============================] - 16s - loss: 2.1820 - acc: 0.2913 - val_loss: 3.1643 - val_acc: 0.2746\n",
      "Epoch 5/100\n",
      "146/146 [==============================] - 16s - loss: 2.1027 - acc: 0.3118 - val_loss: 2.4480 - val_acc: 0.3485\n",
      "Epoch 6/100\n",
      "146/146 [==============================] - 16s - loss: 2.0569 - acc: 0.3199 - val_loss: 2.1591 - val_acc: 0.3800\n",
      "Epoch 7/100\n",
      "146/146 [==============================] - 16s - loss: 2.0133 - acc: 0.3368 - val_loss: 2.1795 - val_acc: 0.4062\n",
      "Epoch 8/100\n",
      "146/146 [==============================] - 16s - loss: 1.9594 - acc: 0.3476 - val_loss: 1.9791 - val_acc: 0.4167\n",
      "Epoch 9/100\n",
      "146/146 [==============================] - 16s - loss: 1.9210 - acc: 0.3641 - val_loss: 1.9558 - val_acc: 0.4342\n",
      "Epoch 10/100\n",
      "146/146 [==============================] - 16s - loss: 1.8816 - acc: 0.3714 - val_loss: 1.9321 - val_acc: 0.4416\n",
      "Epoch 11/100\n",
      "146/146 [==============================] - 16s - loss: 1.8579 - acc: 0.3776 - val_loss: 1.9160 - val_acc: 0.4477\n",
      "Epoch 12/100\n",
      "146/146 [==============================] - 16s - loss: 1.8226 - acc: 0.3934 - val_loss: 1.8829 - val_acc: 0.4617\n",
      "Epoch 13/100\n",
      "146/146 [==============================] - 16s - loss: 1.7866 - acc: 0.4085 - val_loss: 1.9479 - val_acc: 0.4666\n",
      "Epoch 14/100\n",
      "146/146 [==============================] - 16s - loss: 1.7821 - acc: 0.4085 - val_loss: 1.8128 - val_acc: 0.4779\n",
      "Epoch 15/100\n",
      "146/146 [==============================] - 16s - loss: 1.7576 - acc: 0.4155 - val_loss: 1.7795 - val_acc: 0.4792\n",
      "Epoch 16/100\n",
      "146/146 [==============================] - 16s - loss: 1.7081 - acc: 0.4265 - val_loss: 1.7620 - val_acc: 0.5042\n",
      "Epoch 17/100\n",
      "146/146 [==============================] - 16s - loss: 1.6864 - acc: 0.4420 - val_loss: 1.7292 - val_acc: 0.5007\n",
      "Epoch 18/100\n",
      "146/146 [==============================] - 16s - loss: 1.6730 - acc: 0.4422 - val_loss: 1.7137 - val_acc: 0.4976\n",
      "Epoch 19/100\n",
      "146/146 [==============================] - 16s - loss: 1.6815 - acc: 0.4432 - val_loss: 1.7638 - val_acc: 0.4972\n",
      "Epoch 20/100\n",
      "146/146 [==============================] - 16s - loss: 1.6584 - acc: 0.4468 - val_loss: 1.9053 - val_acc: 0.4897\n",
      "Epoch 21/100\n",
      "146/146 [==============================] - 16s - loss: 1.6545 - acc: 0.4427 - val_loss: 1.6881 - val_acc: 0.5081\n",
      "Epoch 22/100\n",
      "146/146 [==============================] - 16s - loss: 1.6268 - acc: 0.4577 - val_loss: 1.6643 - val_acc: 0.5085\n",
      "Epoch 23/100\n",
      "146/146 [==============================] - 16s - loss: 1.5928 - acc: 0.4764 - val_loss: 1.7416 - val_acc: 0.5042\n",
      "Epoch 24/100\n",
      "146/146 [==============================] - 16s - loss: 1.6121 - acc: 0.4702 - val_loss: 1.6644 - val_acc: 0.5055\n",
      "Epoch 25/100\n",
      "146/146 [==============================] - 16s - loss: 1.6097 - acc: 0.4649 - val_loss: 1.6918 - val_acc: 0.5243\n",
      "Epoch 26/100\n",
      "146/146 [==============================] - 16s - loss: 1.5892 - acc: 0.4715 - val_loss: 1.7013 - val_acc: 0.5208\n",
      "Epoch 27/100\n",
      "146/146 [==============================] - 16s - loss: 1.5589 - acc: 0.4761 - val_loss: 1.6677 - val_acc: 0.5138\n",
      "Epoch 28/100\n",
      "146/146 [==============================] - 16s - loss: 1.5691 - acc: 0.4815 - val_loss: 1.6243 - val_acc: 0.5230\n",
      "Epoch 29/100\n",
      "146/146 [==============================] - 16s - loss: 1.5386 - acc: 0.4863 - val_loss: 1.5978 - val_acc: 0.5103\n",
      "Epoch 30/100\n",
      "146/146 [==============================] - 16s - loss: 1.5562 - acc: 0.4866 - val_loss: 1.6029 - val_acc: 0.5238\n",
      "Epoch 31/100\n",
      "146/146 [==============================] - 16s - loss: 1.5274 - acc: 0.4936 - val_loss: 1.5886 - val_acc: 0.5269\n",
      "Epoch 32/100\n",
      "146/146 [==============================] - 16s - loss: 1.5320 - acc: 0.4932 - val_loss: 1.6030 - val_acc: 0.5243\n",
      "Epoch 33/100\n",
      "146/146 [==============================] - 16s - loss: 1.5089 - acc: 0.4963 - val_loss: 1.5424 - val_acc: 0.5326\n",
      "Epoch 34/100\n",
      "146/146 [==============================] - 16s - loss: 1.5119 - acc: 0.5012 - val_loss: 1.5634 - val_acc: 0.5396\n",
      "Epoch 35/100\n",
      "146/146 [==============================] - 16s - loss: 1.4927 - acc: 0.5072 - val_loss: 1.6097 - val_acc: 0.5203\n",
      "Epoch 36/100\n",
      "146/146 [==============================] - 16s - loss: 1.5010 - acc: 0.5013 - val_loss: 1.5113 - val_acc: 0.5479\n",
      "Epoch 37/100\n",
      "146/146 [==============================] - 16s - loss: 1.4905 - acc: 0.5098 - val_loss: 1.6258 - val_acc: 0.5059\n",
      "Epoch 38/100\n",
      "146/146 [==============================] - 16s - loss: 1.4628 - acc: 0.5116 - val_loss: 1.5870 - val_acc: 0.5174\n",
      "Epoch 39/100\n",
      "146/146 [==============================] - 17s - loss: 1.4862 - acc: 0.5086 - val_loss: 1.5547 - val_acc: 0.5426\n",
      "Epoch 40/100\n",
      "146/146 [==============================] - 16s - loss: 1.4616 - acc: 0.5171 - val_loss: 1.5599 - val_acc: 0.5509\n",
      "Epoch 41/100\n",
      "146/146 [==============================] - 16s - loss: 1.4517 - acc: 0.5171 - val_loss: 1.4378 - val_acc: 0.5514\n",
      "Epoch 42/100\n",
      "146/146 [==============================] - 17s - loss: 1.4356 - acc: 0.5226 - val_loss: 1.4716 - val_acc: 0.5601\n",
      "Epoch 43/100\n",
      "146/146 [==============================] - 16s - loss: 1.4437 - acc: 0.5249 - val_loss: 1.4818 - val_acc: 0.5413\n",
      "Epoch 44/100\n",
      "146/146 [==============================] - 16s - loss: 1.4386 - acc: 0.5325 - val_loss: 1.4701 - val_acc: 0.5562\n",
      "Epoch 45/100\n",
      "146/146 [==============================] - 17s - loss: 1.4159 - acc: 0.5396 - val_loss: 1.5068 - val_acc: 0.5531\n",
      "Epoch 46/100\n",
      "146/146 [==============================] - 16s - loss: 1.4111 - acc: 0.5310 - val_loss: 1.4830 - val_acc: 0.5479\n",
      "Epoch 47/100\n",
      "146/146 [==============================] - 16s - loss: 1.4331 - acc: 0.5355 - val_loss: 1.5267 - val_acc: 0.5614\n",
      "Epoch 48/100\n",
      "146/146 [==============================] - 16s - loss: 1.4007 - acc: 0.5378 - val_loss: 1.4570 - val_acc: 0.5439\n",
      "Epoch 49/100\n",
      "146/146 [==============================] - 16s - loss: 1.3750 - acc: 0.5421 - val_loss: 1.4298 - val_acc: 0.5684\n",
      "Epoch 50/100\n",
      "146/146 [==============================] - 16s - loss: 1.4030 - acc: 0.5419 - val_loss: 1.4474 - val_acc: 0.5505\n",
      "Epoch 51/100\n",
      "146/146 [==============================] - 16s - loss: 1.3821 - acc: 0.5440 - val_loss: 1.4691 - val_acc: 0.5501\n",
      "Epoch 52/100\n",
      "146/146 [==============================] - 16s - loss: 1.3712 - acc: 0.5429 - val_loss: 1.4237 - val_acc: 0.5662\n",
      "Epoch 53/100\n",
      "146/146 [==============================] - 16s - loss: 1.3591 - acc: 0.5565 - val_loss: 1.4133 - val_acc: 0.5601\n",
      "Epoch 54/100\n",
      "146/146 [==============================] - 16s - loss: 1.3696 - acc: 0.5519 - val_loss: 1.4731 - val_acc: 0.5488\n",
      "Epoch 55/100\n",
      "146/146 [==============================] - 16s - loss: 1.3751 - acc: 0.5493 - val_loss: 1.4417 - val_acc: 0.5632\n",
      "Epoch 56/100\n",
      "146/146 [==============================] - 16s - loss: 1.3570 - acc: 0.5597 - val_loss: 1.4432 - val_acc: 0.5444\n",
      "Epoch 57/100\n",
      "146/146 [==============================] - 16s - loss: 1.3412 - acc: 0.5650 - val_loss: 1.3970 - val_acc: 0.5654\n",
      "Epoch 58/100\n",
      "146/146 [==============================] - 16s - loss: 1.3515 - acc: 0.5561 - val_loss: 1.3658 - val_acc: 0.5815\n",
      "Epoch 59/100\n",
      "146/146 [==============================] - 16s - loss: 1.3487 - acc: 0.5647 - val_loss: 1.4161 - val_acc: 0.5562\n",
      "Epoch 60/100\n",
      "146/146 [==============================] - 16s - loss: 1.3290 - acc: 0.5675 - val_loss: 1.3213 - val_acc: 0.5864\n",
      "Epoch 61/100\n",
      "146/146 [==============================] - 16s - loss: 1.3251 - acc: 0.5657 - val_loss: 1.3404 - val_acc: 0.5824\n",
      "Epoch 62/100\n",
      "146/146 [==============================] - 16s - loss: 1.3274 - acc: 0.5743 - val_loss: 1.3728 - val_acc: 0.5562\n",
      "Epoch 63/100\n",
      "146/146 [==============================] - 16s - loss: 1.3063 - acc: 0.5672 - val_loss: 1.3647 - val_acc: 0.5815\n",
      "Epoch 64/100\n",
      "146/146 [==============================] - 16s - loss: 1.3210 - acc: 0.5709 - val_loss: 1.3616 - val_acc: 0.5837\n",
      "Epoch 65/100\n",
      "146/146 [==============================] - 16s - loss: 1.3214 - acc: 0.5759 - val_loss: 1.3558 - val_acc: 0.5833\n",
      "Epoch 66/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146/146 [==============================] - 16s - loss: 1.3004 - acc: 0.5770 - val_loss: 1.3269 - val_acc: 0.5885\n",
      "Epoch 67/100\n",
      "146/146 [==============================] - 16s - loss: 1.3038 - acc: 0.5751 - val_loss: 1.3406 - val_acc: 0.5820\n",
      "Epoch 68/100\n",
      "146/146 [==============================] - 16s - loss: 1.2950 - acc: 0.5776 - val_loss: 1.3872 - val_acc: 0.5706\n",
      "Epoch 69/100\n",
      "146/146 [==============================] - 17s - loss: 1.2841 - acc: 0.5810 - val_loss: 1.3007 - val_acc: 0.5964\n",
      "Epoch 70/100\n",
      "146/146 [==============================] - 16s - loss: 1.2853 - acc: 0.5812 - val_loss: 1.3220 - val_acc: 0.5820\n",
      "Epoch 71/100\n",
      "146/146 [==============================] - 16s - loss: 1.2844 - acc: 0.5780 - val_loss: 1.2795 - val_acc: 0.5973\n",
      "Epoch 72/100\n",
      "146/146 [==============================] - 17s - loss: 1.2755 - acc: 0.5858 - val_loss: 1.2444 - val_acc: 0.6091\n",
      "Epoch 73/100\n",
      "146/146 [==============================] - 16s - loss: 1.2784 - acc: 0.5863 - val_loss: 1.2273 - val_acc: 0.6108\n",
      "Epoch 74/100\n",
      "146/146 [==============================] - 16s - loss: 1.2772 - acc: 0.5914 - val_loss: 1.2864 - val_acc: 0.6047\n",
      "Epoch 75/100\n",
      "146/146 [==============================] - 16s - loss: 1.2687 - acc: 0.5804 - val_loss: 1.3154 - val_acc: 0.5829\n",
      "Epoch 76/100\n",
      "146/146 [==============================] - 16s - loss: 1.2538 - acc: 0.5898 - val_loss: 1.3094 - val_acc: 0.6095\n",
      "Epoch 77/100\n",
      "146/146 [==============================] - 16s - loss: 1.2717 - acc: 0.5932 - val_loss: 1.2778 - val_acc: 0.5982\n",
      "Epoch 78/100\n",
      "146/146 [==============================] - 16s - loss: 1.2214 - acc: 0.6079 - val_loss: 1.2992 - val_acc: 0.5837\n",
      "Epoch 79/100\n",
      "146/146 [==============================] - 16s - loss: 1.2500 - acc: 0.5908 - val_loss: 1.2945 - val_acc: 0.5846\n",
      "Epoch 80/100\n",
      "146/146 [==============================] - 17s - loss: 1.2554 - acc: 0.5938 - val_loss: 1.2790 - val_acc: 0.5951\n",
      "Epoch 81/100\n",
      "146/146 [==============================] - 16s - loss: 1.2421 - acc: 0.5940 - val_loss: 1.2863 - val_acc: 0.5947\n",
      "Epoch 82/100\n",
      "146/146 [==============================] - 16s - loss: 1.2341 - acc: 0.5942 - val_loss: 1.2406 - val_acc: 0.6025\n",
      "Epoch 83/100\n",
      "146/146 [==============================] - 16s - loss: 1.2488 - acc: 0.6015 - val_loss: 1.2728 - val_acc: 0.6065\n",
      "Epoch 84/100\n",
      "146/146 [==============================] - 16s - loss: 1.2289 - acc: 0.6046 - val_loss: 1.2446 - val_acc: 0.6126\n",
      "Epoch 85/100\n",
      "146/146 [==============================] - 16s - loss: 1.2412 - acc: 0.6012 - val_loss: 1.1847 - val_acc: 0.6253\n",
      "Epoch 86/100\n",
      "146/146 [==============================] - 16s - loss: 1.2155 - acc: 0.6105 - val_loss: 1.2075 - val_acc: 0.6152\n",
      "Epoch 87/100\n",
      "146/146 [==============================] - 16s - loss: 1.2125 - acc: 0.6084 - val_loss: 1.2380 - val_acc: 0.6030\n",
      "Epoch 88/100\n",
      "146/146 [==============================] - 16s - loss: 1.2262 - acc: 0.5992 - val_loss: 1.2375 - val_acc: 0.6008\n",
      "Epoch 89/100\n",
      "146/146 [==============================] - 16s - loss: 1.2267 - acc: 0.6070 - val_loss: 1.1976 - val_acc: 0.6082\n",
      "Epoch 90/100\n",
      "146/146 [==============================] - 16s - loss: 1.2208 - acc: 0.6006 - val_loss: 1.2135 - val_acc: 0.6178\n",
      "Epoch 91/100\n",
      "146/146 [==============================] - 16s - loss: 1.2024 - acc: 0.6132 - val_loss: 1.2258 - val_acc: 0.6122\n",
      "Epoch 92/100\n",
      "146/146 [==============================] - 16s - loss: 1.2192 - acc: 0.6064 - val_loss: 1.2522 - val_acc: 0.5859\n",
      "Epoch 93/100\n",
      "146/146 [==============================] - 16s - loss: 1.1820 - acc: 0.6138 - val_loss: 1.1349 - val_acc: 0.6288\n",
      "Epoch 94/100\n",
      "146/146 [==============================] - 16s - loss: 1.2013 - acc: 0.6068 - val_loss: 1.2176 - val_acc: 0.6200\n",
      "Epoch 95/100\n",
      "146/146 [==============================] - 16s - loss: 1.1940 - acc: 0.6183 - val_loss: 1.1551 - val_acc: 0.6314\n",
      "Epoch 96/100\n",
      "146/146 [==============================] - 16s - loss: 1.1956 - acc: 0.6102 - val_loss: 1.1933 - val_acc: 0.6174\n",
      "Epoch 97/100\n",
      "146/146 [==============================] - 16s - loss: 1.1918 - acc: 0.6153 - val_loss: 1.1825 - val_acc: 0.6165\n",
      "Epoch 98/100\n",
      "146/146 [==============================] - 16s - loss: 1.1909 - acc: 0.6202 - val_loss: 1.1412 - val_acc: 0.6371\n",
      "Epoch 99/100\n",
      "146/146 [==============================] - 16s - loss: 1.1786 - acc: 0.6250 - val_loss: 1.2526 - val_acc: 0.6034\n",
      "Epoch 100/100\n",
      "146/146 [==============================] - 16s - loss: 1.1897 - acc: 0.6119 - val_loss: 1.1827 - val_acc: 0.6253\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f142506f390>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn6.fit_generator(train_generator, steps_per_epoch=X_train.shape[0] // 64, epochs=100, \n",
    "                    validation_data=test_generator, validation_steps=len(y_test_1hot)//64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "pickle.dump(cnn2, open('cnn2.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "loaded_model2 = pickle.load(open('cnn2.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#_________________________________________________________________try2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn7 = Sequential([\n",
    "    Conv2D(40, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=(100,100,3)),\n",
    "#     Dropout(.25),\n",
    "    MaxPool2D((2,2)),\n",
    "    \n",
    "    Conv2D(50, (3, 3), activation='relu'),\n",
    "    BatchNormalization(axis=-1),\n",
    "    Dropout(.25),\n",
    "    MaxPool2D((2,2)),\n",
    "    \n",
    "    Conv2D(60, (3, 3), activation='relu'),\n",
    "    BatchNormalization(axis=-1),\n",
    "    Dropout(.5),\n",
    "    MaxPool2D((2,2)),\n",
    "    \n",
    "#     Conv2D(150, (3, 3), activation='relu'),\n",
    "#     BatchNormalization(axis=-1),\n",
    "#     Dropout(.25),\n",
    "#     MaxPool2D((2,2)),\n",
    "    \n",
    "#     Conv2D(100, (3, 3), activation='relu'),\n",
    "#     Dropout(.25),\n",
    "#     MaxPool2D((2,2)),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(axis=-1),\n",
    "    Dropout(.5),\n",
    "    Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_19 (Conv2D)           (None, 98, 98, 40)        1120      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 49, 49, 40)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 47, 47, 50)        18050     \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 47, 47, 50)        200       \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 47, 47, 50)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 23, 23, 50)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 21, 21, 60)        27060     \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 21, 21, 60)        240       \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 21, 21, 60)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 10, 10, 60)        0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 6000)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 64)                384064    \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 431,640\n",
      "Trainable params: 431,292\n",
      "Non-trainable params: 348\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn7.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn7.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(lr=0.1),\n",
    "#               optimizer=Adam(lr=0.2),\n",
    "#               optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "imgen_train2 = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=90,\n",
    "    width_shift_range=0.5,\n",
    "    height_shift_range=0.5,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.4,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    ")\n",
    "\n",
    "imgen_test2=ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_generator2 = imgen_train2.flow(X_train, y_train_1hot, batch_size=64)\n",
    "test_generator2 = imgen_test2.flow(X_test, y_test_1hot, batch_size=64)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "146/146 [==============================] - 18s - loss: 3.0969 - acc: 0.1575 - val_loss: 2.5419 - val_acc: 0.1068\n",
      "Epoch 2/100\n",
      "146/146 [==============================] - 16s - loss: 2.6768 - acc: 0.2140 - val_loss: 2.6436 - val_acc: 0.1220\n",
      "Epoch 3/100\n",
      "146/146 [==============================] - 17s - loss: 2.5251 - acc: 0.2390 - val_loss: 2.4145 - val_acc: 0.1876\n",
      "Epoch 4/100\n",
      "146/146 [==============================] - 16s - loss: 2.3962 - acc: 0.2668 - val_loss: 1.9925 - val_acc: 0.3183\n",
      "Epoch 5/100\n",
      "146/146 [==============================] - 16s - loss: 2.2820 - acc: 0.2882 - val_loss: 1.8591 - val_acc: 0.3551\n",
      "Epoch 6/100\n",
      "146/146 [==============================] - 16s - loss: 2.2128 - acc: 0.2995 - val_loss: 1.8094 - val_acc: 0.3874\n",
      "Epoch 7/100\n",
      "146/146 [==============================] - 16s - loss: 2.1674 - acc: 0.3111 - val_loss: 1.7475 - val_acc: 0.4220\n",
      "Epoch 8/100\n",
      "146/146 [==============================] - 16s - loss: 2.1253 - acc: 0.3215 - val_loss: 1.7259 - val_acc: 0.4215\n",
      "Epoch 9/100\n",
      "146/146 [==============================] - 16s - loss: 2.0816 - acc: 0.3300 - val_loss: 1.7143 - val_acc: 0.4386\n",
      "Epoch 10/100\n",
      "146/146 [==============================] - 16s - loss: 2.0222 - acc: 0.3385 - val_loss: 1.7125 - val_acc: 0.4250\n",
      "Epoch 11/100\n",
      "146/146 [==============================] - 16s - loss: 2.0035 - acc: 0.3535 - val_loss: 1.7185 - val_acc: 0.4224\n",
      "Epoch 12/100\n",
      "146/146 [==============================] - 16s - loss: 1.9850 - acc: 0.3548 - val_loss: 1.7275 - val_acc: 0.4324\n",
      "Epoch 13/100\n",
      "146/146 [==============================] - 16s - loss: 1.9651 - acc: 0.3683 - val_loss: 1.6704 - val_acc: 0.4521\n",
      "Epoch 14/100\n",
      "146/146 [==============================] - 16s - loss: 1.9286 - acc: 0.3741 - val_loss: 1.6797 - val_acc: 0.4447\n",
      "Epoch 15/100\n",
      "146/146 [==============================] - 16s - loss: 1.9034 - acc: 0.3824 - val_loss: 1.6743 - val_acc: 0.4508\n",
      "Epoch 16/100\n",
      "146/146 [==============================] - 16s - loss: 1.8939 - acc: 0.3838 - val_loss: 1.6691 - val_acc: 0.4600\n",
      "Epoch 17/100\n",
      "146/146 [==============================] - 16s - loss: 1.8791 - acc: 0.3903 - val_loss: 1.6141 - val_acc: 0.4587\n",
      "Epoch 18/100\n",
      "146/146 [==============================] - 16s - loss: 1.8371 - acc: 0.4048 - val_loss: 1.6687 - val_acc: 0.4534\n",
      "Epoch 19/100\n",
      "146/146 [==============================] - 16s - loss: 1.8321 - acc: 0.3972 - val_loss: 1.6748 - val_acc: 0.4661\n",
      "Epoch 20/100\n",
      "146/146 [==============================] - 16s - loss: 1.8188 - acc: 0.4075 - val_loss: 1.6090 - val_acc: 0.4744\n",
      "Epoch 21/100\n",
      "146/146 [==============================] - 16s - loss: 1.7823 - acc: 0.4244 - val_loss: 1.6045 - val_acc: 0.4740\n",
      "Epoch 22/100\n",
      "146/146 [==============================] - 16s - loss: 1.7847 - acc: 0.4178 - val_loss: 1.5461 - val_acc: 0.4998\n",
      "Epoch 23/100\n",
      "146/146 [==============================] - 17s - loss: 1.7744 - acc: 0.4276 - val_loss: 1.6802 - val_acc: 0.4609\n",
      "Epoch 24/100\n",
      "146/146 [==============================] - 16s - loss: 1.7506 - acc: 0.4330 - val_loss: 1.6244 - val_acc: 0.4644\n",
      "Epoch 25/100\n",
      "146/146 [==============================] - 16s - loss: 1.7564 - acc: 0.4278 - val_loss: 1.5798 - val_acc: 0.4770\n",
      "Epoch 26/100\n",
      "146/146 [==============================] - 16s - loss: 1.7140 - acc: 0.4409 - val_loss: 1.6001 - val_acc: 0.4762\n",
      "Epoch 27/100\n",
      "146/146 [==============================] - 16s - loss: 1.7081 - acc: 0.4393 - val_loss: 1.6323 - val_acc: 0.4652\n",
      "Epoch 28/100\n",
      "146/146 [==============================] - 16s - loss: 1.7261 - acc: 0.4366 - val_loss: 1.6189 - val_acc: 0.4827\n",
      "Epoch 29/100\n",
      "146/146 [==============================] - 17s - loss: 1.7008 - acc: 0.4478 - val_loss: 1.5840 - val_acc: 0.4889\n",
      "Epoch 30/100\n",
      "146/146 [==============================] - 16s - loss: 1.6862 - acc: 0.4505 - val_loss: 1.5617 - val_acc: 0.4867\n",
      "Epoch 31/100\n",
      "146/146 [==============================] - 16s - loss: 1.6809 - acc: 0.4499 - val_loss: 1.5298 - val_acc: 0.5111\n",
      "Epoch 32/100\n",
      "146/146 [==============================] - 16s - loss: 1.6718 - acc: 0.4601 - val_loss: 1.5695 - val_acc: 0.4985\n",
      "Epoch 33/100\n",
      "146/146 [==============================] - 16s - loss: 1.6668 - acc: 0.4568 - val_loss: 1.5800 - val_acc: 0.4810\n",
      "Epoch 34/100\n",
      "146/146 [==============================] - 16s - loss: 1.6395 - acc: 0.4671 - val_loss: 1.6058 - val_acc: 0.4854\n",
      "Epoch 35/100\n",
      "146/146 [==============================] - 16s - loss: 1.6123 - acc: 0.4674 - val_loss: 1.5198 - val_acc: 0.4941\n",
      "Epoch 36/100\n",
      "146/146 [==============================] - 16s - loss: 1.6653 - acc: 0.4643 - val_loss: 1.5686 - val_acc: 0.4932\n",
      "Epoch 37/100\n",
      "146/146 [==============================] - 16s - loss: 1.6219 - acc: 0.4706 - val_loss: 1.5649 - val_acc: 0.4941\n",
      "Epoch 38/100\n",
      "146/146 [==============================] - 16s - loss: 1.6084 - acc: 0.4740 - val_loss: 1.4928 - val_acc: 0.5156\n",
      "Epoch 39/100\n",
      "146/146 [==============================] - 16s - loss: 1.6148 - acc: 0.4750 - val_loss: 1.4905 - val_acc: 0.5247\n",
      "Epoch 40/100\n",
      "146/146 [==============================] - 16s - loss: 1.5892 - acc: 0.4838 - val_loss: 1.5400 - val_acc: 0.5072\n",
      "Epoch 41/100\n",
      "146/146 [==============================] - 16s - loss: 1.5953 - acc: 0.4840 - val_loss: 1.6287 - val_acc: 0.4862\n",
      "Epoch 42/100\n",
      "146/146 [==============================] - 17s - loss: 1.5723 - acc: 0.4812 - val_loss: 1.4657 - val_acc: 0.5300\n",
      "Epoch 43/100\n",
      "146/146 [==============================] - 16s - loss: 1.5756 - acc: 0.4906 - val_loss: 1.5384 - val_acc: 0.5230\n",
      "Epoch 44/100\n",
      "146/146 [==============================] - 16s - loss: 1.5726 - acc: 0.4835 - val_loss: 1.5129 - val_acc: 0.4985\n",
      "Epoch 45/100\n",
      "146/146 [==============================] - 17s - loss: 1.5681 - acc: 0.4851 - val_loss: 1.5589 - val_acc: 0.5085\n",
      "Epoch 46/100\n",
      "146/146 [==============================] - 16s - loss: 1.5760 - acc: 0.4879 - val_loss: 1.5170 - val_acc: 0.5203\n",
      "Epoch 47/100\n",
      "146/146 [==============================] - 16s - loss: 1.5613 - acc: 0.4903 - val_loss: 1.5161 - val_acc: 0.5055\n",
      "Epoch 48/100\n",
      "146/146 [==============================] - 17s - loss: 1.5405 - acc: 0.4950 - val_loss: 1.4947 - val_acc: 0.5042\n",
      "Epoch 49/100\n",
      "146/146 [==============================] - 16s - loss: 1.5331 - acc: 0.4987 - val_loss: 1.4754 - val_acc: 0.5321\n",
      "Epoch 50/100\n",
      "146/146 [==============================] - 16s - loss: 1.5280 - acc: 0.4970 - val_loss: 1.4287 - val_acc: 0.5225\n",
      "Epoch 51/100\n",
      "146/146 [==============================] - 17s - loss: 1.5162 - acc: 0.5040 - val_loss: 1.4212 - val_acc: 0.5457\n",
      "Epoch 52/100\n",
      "146/146 [==============================] - 16s - loss: 1.5045 - acc: 0.5026 - val_loss: 1.5072 - val_acc: 0.5059\n",
      "Epoch 53/100\n",
      "146/146 [==============================] - 16s - loss: 1.5418 - acc: 0.4976 - val_loss: 1.4849 - val_acc: 0.5203\n",
      "Epoch 54/100\n",
      "146/146 [==============================] - 16s - loss: 1.5251 - acc: 0.5041 - val_loss: 1.4641 - val_acc: 0.5348\n",
      "Epoch 55/100\n",
      "146/146 [==============================] - 17s - loss: 1.5100 - acc: 0.5105 - val_loss: 1.3844 - val_acc: 0.5614\n",
      "Epoch 56/100\n",
      "146/146 [==============================] - 16s - loss: 1.4915 - acc: 0.5074 - val_loss: 1.4316 - val_acc: 0.5334\n",
      "Epoch 57/100\n",
      "146/146 [==============================] - 17s - loss: 1.4873 - acc: 0.5136 - val_loss: 1.3881 - val_acc: 0.5557\n",
      "Epoch 58/100\n",
      "146/146 [==============================] - 16s - loss: 1.4699 - acc: 0.5214 - val_loss: 1.4476 - val_acc: 0.5282\n",
      "Epoch 59/100\n",
      "146/146 [==============================] - 16s - loss: 1.4755 - acc: 0.5140 - val_loss: 1.5107 - val_acc: 0.5103\n",
      "Epoch 60/100\n",
      "146/146 [==============================] - 16s - loss: 1.4750 - acc: 0.5109 - val_loss: 1.3914 - val_acc: 0.5501\n",
      "Epoch 61/100\n",
      "146/146 [==============================] - 16s - loss: 1.4699 - acc: 0.5212 - val_loss: 1.4302 - val_acc: 0.5208\n",
      "Epoch 62/100\n",
      "146/146 [==============================] - 16s - loss: 1.4722 - acc: 0.5243 - val_loss: 1.4422 - val_acc: 0.5356\n",
      "Epoch 63/100\n",
      "146/146 [==============================] - 16s - loss: 1.4613 - acc: 0.5181 - val_loss: 1.4297 - val_acc: 0.5348\n",
      "Epoch 64/100\n",
      "146/146 [==============================] - 16s - loss: 1.4434 - acc: 0.5324 - val_loss: 1.3968 - val_acc: 0.5413\n",
      "Epoch 65/100\n",
      "146/146 [==============================] - 17s - loss: 1.4478 - acc: 0.5246 - val_loss: 1.4956 - val_acc: 0.5230\n",
      "Epoch 66/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146/146 [==============================] - 17s - loss: 1.4430 - acc: 0.5245 - val_loss: 1.4284 - val_acc: 0.5265\n",
      "Epoch 67/100\n",
      "146/146 [==============================] - 16s - loss: 1.4504 - acc: 0.5271 - val_loss: 1.3646 - val_acc: 0.5553\n",
      "Epoch 68/100\n",
      "146/146 [==============================] - 16s - loss: 1.4258 - acc: 0.5416 - val_loss: 1.4601 - val_acc: 0.5168\n",
      "Epoch 69/100\n",
      "146/146 [==============================] - 17s - loss: 1.4479 - acc: 0.5239 - val_loss: 1.3525 - val_acc: 0.5676\n",
      "Epoch 70/100\n",
      "146/146 [==============================] - 16s - loss: 1.4234 - acc: 0.5290 - val_loss: 1.3761 - val_acc: 0.5514\n",
      "Epoch 71/100\n",
      "146/146 [==============================] - 16s - loss: 1.4204 - acc: 0.5280 - val_loss: 1.3738 - val_acc: 0.5645\n",
      "Epoch 72/100\n",
      "146/146 [==============================] - 16s - loss: 1.4130 - acc: 0.5335 - val_loss: 1.3375 - val_acc: 0.5667\n",
      "Epoch 73/100\n",
      "146/146 [==============================] - 16s - loss: 1.4214 - acc: 0.5312 - val_loss: 1.3867 - val_acc: 0.5348\n",
      "Epoch 74/100\n",
      "146/146 [==============================] - 16s - loss: 1.4009 - acc: 0.5415 - val_loss: 1.3513 - val_acc: 0.5689\n",
      "Epoch 75/100\n",
      "146/146 [==============================] - 16s - loss: 1.3898 - acc: 0.5429 - val_loss: 1.4442 - val_acc: 0.5304\n",
      "Epoch 76/100\n",
      "146/146 [==============================] - 16s - loss: 1.4146 - acc: 0.5328 - val_loss: 1.3751 - val_acc: 0.5479\n",
      "Epoch 77/100\n",
      "146/146 [==============================] - 16s - loss: 1.4245 - acc: 0.5297 - val_loss: 1.4775 - val_acc: 0.5212\n",
      "Epoch 78/100\n",
      "146/146 [==============================] - 17s - loss: 1.3985 - acc: 0.5464 - val_loss: 1.4076 - val_acc: 0.5387\n",
      "Epoch 79/100\n",
      "146/146 [==============================] - 17s - loss: 1.4198 - acc: 0.5321 - val_loss: 1.3248 - val_acc: 0.5719\n",
      "Epoch 80/100\n",
      "146/146 [==============================] - 16s - loss: 1.3822 - acc: 0.5505 - val_loss: 1.4372 - val_acc: 0.5435\n",
      "Epoch 81/100\n",
      "146/146 [==============================] - 16s - loss: 1.3910 - acc: 0.5406 - val_loss: 1.3797 - val_acc: 0.5509\n",
      "Epoch 82/100\n",
      "146/146 [==============================] - 16s - loss: 1.3923 - acc: 0.5437 - val_loss: 1.4097 - val_acc: 0.5361\n",
      "Epoch 83/100\n",
      "146/146 [==============================] - 17s - loss: 1.3693 - acc: 0.5535 - val_loss: 1.3682 - val_acc: 0.5470\n",
      "Epoch 84/100\n",
      "146/146 [==============================] - 16s - loss: 1.3917 - acc: 0.5418 - val_loss: 1.4251 - val_acc: 0.5321\n",
      "Epoch 85/100\n",
      "146/146 [==============================] - 16s - loss: 1.3444 - acc: 0.5665 - val_loss: 1.3221 - val_acc: 0.5719\n",
      "Epoch 86/100\n",
      "146/146 [==============================] - 16s - loss: 1.3730 - acc: 0.5550 - val_loss: 1.3589 - val_acc: 0.5557\n",
      "Epoch 87/100\n",
      "146/146 [==============================] - 16s - loss: 1.3690 - acc: 0.5515 - val_loss: 1.3409 - val_acc: 0.5623\n",
      "Epoch 88/100\n",
      "146/146 [==============================] - 16s - loss: 1.3571 - acc: 0.5495 - val_loss: 1.4253 - val_acc: 0.5387\n",
      "Epoch 89/100\n",
      "146/146 [==============================] - 16s - loss: 1.3718 - acc: 0.5502 - val_loss: 1.3256 - val_acc: 0.5802\n",
      "Epoch 90/100\n",
      "146/146 [==============================] - 16s - loss: 1.3573 - acc: 0.5661 - val_loss: 1.3296 - val_acc: 0.5689\n",
      "Epoch 91/100\n",
      "146/146 [==============================] - 16s - loss: 1.3623 - acc: 0.5560 - val_loss: 1.3025 - val_acc: 0.5780\n",
      "Epoch 92/100\n",
      "146/146 [==============================] - 16s - loss: 1.3579 - acc: 0.5591 - val_loss: 1.2948 - val_acc: 0.5855\n",
      "Epoch 93/100\n",
      "146/146 [==============================] - 16s - loss: 1.3482 - acc: 0.5567 - val_loss: 1.3370 - val_acc: 0.5728\n",
      "Epoch 94/100\n",
      "146/146 [==============================] - 16s - loss: 1.3586 - acc: 0.5616 - val_loss: 1.3911 - val_acc: 0.5501\n",
      "Epoch 95/100\n",
      "146/146 [==============================] - 16s - loss: 1.3446 - acc: 0.5572 - val_loss: 1.2976 - val_acc: 0.5785\n",
      "Epoch 96/100\n",
      "146/146 [==============================] - 16s - loss: 1.3493 - acc: 0.5622 - val_loss: 1.3194 - val_acc: 0.5649\n",
      "Epoch 97/100\n",
      "146/146 [==============================] - 16s - loss: 1.3303 - acc: 0.5680 - val_loss: 1.2890 - val_acc: 0.5907\n",
      "Epoch 98/100\n",
      "146/146 [==============================] - 16s - loss: 1.3570 - acc: 0.5603 - val_loss: 1.2809 - val_acc: 0.5890\n",
      "Epoch 99/100\n",
      "146/146 [==============================] - 16s - loss: 1.3279 - acc: 0.5717 - val_loss: 1.3182 - val_acc: 0.5719\n",
      "Epoch 100/100\n",
      "146/146 [==============================] - 16s - loss: 1.3371 - acc: 0.5650 - val_loss: 1.3118 - val_acc: 0.5807\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f14238965d0>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn7.fit_generator(train_generator2, steps_per_epoch=X_train.shape[0] // 64, epochs=100, \n",
    "                    validation_data=test_generator2, validation_steps=len(y_test_1hot)//64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#try3____________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn8 = Sequential([\n",
    "    Conv2D(30, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=(100,100,3)),\n",
    "    Dropout(.25),\n",
    "    MaxPool2D((2,2)),\n",
    "    \n",
    "    Conv2D(60, (3, 3), activation='relu'),\n",
    "    BatchNormalization(axis=-1),\n",
    "    Dropout(.5),\n",
    "    MaxPool2D((2,2)),\n",
    "    \n",
    "#     Conv2D(60, (3, 3), activation='relu'),\n",
    "#     BatchNormalization(axis=-1),\n",
    "#     Dropout(.5),\n",
    "#     MaxPool2D((2,2)),\n",
    "    \n",
    "#     Conv2D(150, (3, 3), activation='relu'),\n",
    "#     BatchNormalization(axis=-1),\n",
    "#     Dropout(.25),\n",
    "#     MaxPool2D((2,2)),\n",
    "    \n",
    "#     Conv2D(100, (3, 3), activation='relu'),\n",
    "#     Dropout(.25),\n",
    "#     MaxPool2D((2,2)),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(axis=-1),\n",
    "    Dropout(.5),\n",
    "    Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_22 (Conv2D)           (None, 98, 98, 30)        840       \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 98, 98, 30)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 49, 49, 30)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 47, 47, 60)        16260     \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 47, 47, 60)        240       \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 47, 47, 60)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 23, 23, 60)        0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 31740)             0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 64)                2031424   \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 2,049,670\n",
      "Trainable params: 2,049,422\n",
      "Non-trainable params: 248\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn8.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn8.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(lr=0.1),\n",
    "#               optimizer=Adam(lr=0.2),\n",
    "#               optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imgen_train2 = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=90,\n",
    "    width_shift_range=0.5,\n",
    "    height_shift_range=0.5,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.4,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    ")\n",
    "\n",
    "imgen_test2=ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_generator3 = imgen_train2.flow(X_train, y_train_1hot, batch_size=64)\n",
    "test_generator3 = imgen_test2.flow(X_test, y_test_1hot, batch_size=64)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "146/146 [==============================] - 18s - loss: 2.7684 - acc: 0.1984 - val_loss: 2.3207 - val_acc: 0.1289\n",
      "Epoch 2/100\n",
      "146/146 [==============================] - 16s - loss: 2.4198 - acc: 0.2495 - val_loss: 2.3140 - val_acc: 0.1570\n",
      "Epoch 3/100\n",
      "146/146 [==============================] - 16s - loss: 2.3584 - acc: 0.2695 - val_loss: 2.5574 - val_acc: 0.1920\n",
      "Epoch 4/100\n",
      "146/146 [==============================] - 16s - loss: 2.2620 - acc: 0.2876 - val_loss: 3.1564 - val_acc: 0.2462\n",
      "Epoch 5/100\n",
      "146/146 [==============================] - 16s - loss: 2.2171 - acc: 0.2969 - val_loss: 3.6256 - val_acc: 0.2615\n",
      "Epoch 6/100\n",
      "146/146 [==============================] - 16s - loss: 2.1525 - acc: 0.3074 - val_loss: 3.7699 - val_acc: 0.2768\n",
      "Epoch 7/100\n",
      "146/146 [==============================] - 16s - loss: 2.1013 - acc: 0.3178 - val_loss: 3.8244 - val_acc: 0.2978\n",
      "Epoch 8/100\n",
      "146/146 [==============================] - 16s - loss: 2.0654 - acc: 0.3353 - val_loss: 4.4172 - val_acc: 0.2833\n",
      "Epoch 9/100\n",
      "146/146 [==============================] - 16s - loss: 2.0288 - acc: 0.3385 - val_loss: 3.8415 - val_acc: 0.3100\n",
      "Epoch 10/100\n",
      "146/146 [==============================] - 16s - loss: 1.9938 - acc: 0.3531 - val_loss: 4.0000 - val_acc: 0.3218\n",
      "Epoch 11/100\n",
      "146/146 [==============================] - 16s - loss: 1.9883 - acc: 0.3599 - val_loss: 3.7746 - val_acc: 0.3288\n",
      "Epoch 12/100\n",
      "146/146 [==============================] - 16s - loss: 1.9407 - acc: 0.3714 - val_loss: 3.6726 - val_acc: 0.3358\n",
      "Epoch 13/100\n",
      "146/146 [==============================] - 16s - loss: 1.9180 - acc: 0.3775 - val_loss: 3.4367 - val_acc: 0.3564\n",
      "Epoch 14/100\n",
      "146/146 [==============================] - 16s - loss: 1.9043 - acc: 0.3837 - val_loss: 3.7955 - val_acc: 0.3467\n",
      "Epoch 15/100\n",
      "146/146 [==============================] - 16s - loss: 1.8611 - acc: 0.3902 - val_loss: 3.3009 - val_acc: 0.3835\n",
      "Epoch 16/100\n",
      "146/146 [==============================] - 16s - loss: 1.8596 - acc: 0.3933 - val_loss: 3.6429 - val_acc: 0.3682\n",
      "Epoch 17/100\n",
      "146/146 [==============================] - 16s - loss: 1.8434 - acc: 0.3971 - val_loss: 3.2637 - val_acc: 0.3795\n",
      "Epoch 18/100\n",
      "146/146 [==============================] - 16s - loss: 1.8327 - acc: 0.4067 - val_loss: 3.4262 - val_acc: 0.3787\n",
      "Epoch 19/100\n",
      "146/146 [==============================] - 16s - loss: 1.8261 - acc: 0.4108 - val_loss: 3.4025 - val_acc: 0.3669\n",
      "Epoch 20/100\n",
      "146/146 [==============================] - 16s - loss: 1.8065 - acc: 0.4090 - val_loss: 3.3879 - val_acc: 0.3813\n",
      "Epoch 21/100\n",
      "146/146 [==============================] - 16s - loss: 1.7994 - acc: 0.4167 - val_loss: 3.5464 - val_acc: 0.3878\n",
      "Epoch 22/100\n",
      "146/146 [==============================] - 16s - loss: 1.7818 - acc: 0.4233 - val_loss: 3.4990 - val_acc: 0.4136\n",
      "Epoch 23/100\n",
      "146/146 [==============================] - 16s - loss: 1.7786 - acc: 0.4172 - val_loss: 3.7431 - val_acc: 0.3808\n",
      "Epoch 24/100\n",
      "146/146 [==============================] - 16s - loss: 1.7465 - acc: 0.4343 - val_loss: 3.1917 - val_acc: 0.4075\n",
      "Epoch 25/100\n",
      "146/146 [==============================] - 16s - loss: 1.7223 - acc: 0.4375 - val_loss: 3.5857 - val_acc: 0.3922\n",
      "Epoch 26/100\n",
      "146/146 [==============================] - 16s - loss: 1.7308 - acc: 0.4325 - val_loss: 3.4202 - val_acc: 0.4062\n",
      "Epoch 27/100\n",
      "146/146 [==============================] - 16s - loss: 1.7244 - acc: 0.4409 - val_loss: 3.5289 - val_acc: 0.4036\n",
      "Epoch 28/100\n",
      "146/146 [==============================] - 16s - loss: 1.7082 - acc: 0.4414 - val_loss: 3.6302 - val_acc: 0.3695\n",
      "Epoch 29/100\n",
      "146/146 [==============================] - 16s - loss: 1.6699 - acc: 0.4503 - val_loss: 3.6685 - val_acc: 0.3865\n",
      "Epoch 30/100\n",
      "146/146 [==============================] - 16s - loss: 1.6839 - acc: 0.4519 - val_loss: 3.3456 - val_acc: 0.3983\n",
      "Epoch 31/100\n",
      "146/146 [==============================] - 16s - loss: 1.6884 - acc: 0.4518 - val_loss: 3.3654 - val_acc: 0.3922\n",
      "Epoch 32/100\n",
      "146/146 [==============================] - 16s - loss: 1.6713 - acc: 0.4580 - val_loss: 3.3558 - val_acc: 0.3782\n",
      "Epoch 33/100\n",
      "146/146 [==============================] - 16s - loss: 1.6535 - acc: 0.4592 - val_loss: 3.5139 - val_acc: 0.3966\n",
      "Epoch 34/100\n",
      "146/146 [==============================] - 16s - loss: 1.6839 - acc: 0.4526 - val_loss: 3.5892 - val_acc: 0.4036\n",
      "Epoch 35/100\n",
      "146/146 [==============================] - 16s - loss: 1.6509 - acc: 0.4606 - val_loss: 3.5390 - val_acc: 0.3918\n",
      "Epoch 36/100\n",
      "146/146 [==============================] - 16s - loss: 1.6594 - acc: 0.4559 - val_loss: 3.6384 - val_acc: 0.3896\n",
      "Epoch 37/100\n",
      "146/146 [==============================] - 16s - loss: 1.6371 - acc: 0.4589 - val_loss: 3.5003 - val_acc: 0.4005\n",
      "Epoch 38/100\n",
      "146/146 [==============================] - 16s - loss: 1.6181 - acc: 0.4741 - val_loss: 3.3577 - val_acc: 0.3997\n",
      "Epoch 39/100\n",
      "146/146 [==============================] - 16s - loss: 1.6118 - acc: 0.4666 - val_loss: 3.4756 - val_acc: 0.4088\n",
      "Epoch 40/100\n",
      "146/146 [==============================] - 16s - loss: 1.6296 - acc: 0.4657 - val_loss: 3.8563 - val_acc: 0.3756\n",
      "Epoch 41/100\n",
      "146/146 [==============================] - 16s - loss: 1.6059 - acc: 0.4759 - val_loss: 3.5442 - val_acc: 0.3948\n",
      "Epoch 42/100\n",
      "146/146 [==============================] - 16s - loss: 1.6111 - acc: 0.4681 - val_loss: 3.3388 - val_acc: 0.3900\n",
      "Epoch 43/100\n",
      "146/146 [==============================] - 16s - loss: 1.6148 - acc: 0.4723 - val_loss: 3.8352 - val_acc: 0.3800\n",
      "Epoch 44/100\n",
      "146/146 [==============================] - 16s - loss: 1.5882 - acc: 0.4801 - val_loss: 3.4072 - val_acc: 0.3953\n",
      "Epoch 45/100\n",
      "146/146 [==============================] - 16s - loss: 1.5848 - acc: 0.4890 - val_loss: 3.5170 - val_acc: 0.4093\n",
      "Epoch 46/100\n",
      "146/146 [==============================] - 16s - loss: 1.5733 - acc: 0.4832 - val_loss: 3.5113 - val_acc: 0.4001\n",
      "Epoch 47/100\n",
      "146/146 [==============================] - 16s - loss: 1.5741 - acc: 0.4760 - val_loss: 3.5062 - val_acc: 0.4014\n",
      "Epoch 48/100\n",
      "146/146 [==============================] - 16s - loss: 1.5750 - acc: 0.4791 - val_loss: 3.5419 - val_acc: 0.3922\n",
      "Epoch 49/100\n",
      "146/146 [==============================] - 16s - loss: 1.5500 - acc: 0.4827 - val_loss: 3.2589 - val_acc: 0.4106\n",
      "Epoch 50/100\n",
      "146/146 [==============================] - 16s - loss: 1.5628 - acc: 0.4915 - val_loss: 3.7969 - val_acc: 0.3966\n",
      "Epoch 51/100\n",
      "146/146 [==============================] - 16s - loss: 1.5324 - acc: 0.4957 - val_loss: 3.5698 - val_acc: 0.3857\n",
      "Epoch 52/100\n",
      "146/146 [==============================] - 16s - loss: 1.5621 - acc: 0.4890 - val_loss: 3.4637 - val_acc: 0.4084\n",
      "Epoch 53/100\n",
      "146/146 [==============================] - 16s - loss: 1.5303 - acc: 0.4995 - val_loss: 3.6758 - val_acc: 0.3970\n",
      "Epoch 54/100\n",
      "146/146 [==============================] - 16s - loss: 1.5450 - acc: 0.4948 - val_loss: 3.6198 - val_acc: 0.4045\n",
      "Epoch 55/100\n",
      "146/146 [==============================] - 16s - loss: 1.5445 - acc: 0.4960 - val_loss: 3.4864 - val_acc: 0.4010\n",
      "Epoch 56/100\n",
      "146/146 [==============================] - 16s - loss: 1.5173 - acc: 0.5003 - val_loss: 3.4136 - val_acc: 0.4075\n",
      "Epoch 57/100\n",
      "146/146 [==============================] - 16s - loss: 1.5245 - acc: 0.5029 - val_loss: 3.3666 - val_acc: 0.4049\n",
      "Epoch 58/100\n",
      "146/146 [==============================] - 16s - loss: 1.5074 - acc: 0.5039 - val_loss: 3.8323 - val_acc: 0.4071\n",
      "Epoch 59/100\n",
      "146/146 [==============================] - 16s - loss: 1.4980 - acc: 0.5059 - val_loss: 3.5377 - val_acc: 0.3935\n",
      "Epoch 60/100\n",
      "146/146 [==============================] - 16s - loss: 1.5033 - acc: 0.4988 - val_loss: 3.5559 - val_acc: 0.4014\n",
      "Epoch 61/100\n",
      "146/146 [==============================] - 16s - loss: 1.5077 - acc: 0.5052 - val_loss: 3.7957 - val_acc: 0.3830\n",
      "Epoch 62/100\n",
      "146/146 [==============================] - 16s - loss: 1.5000 - acc: 0.5110 - val_loss: 3.6461 - val_acc: 0.3905\n",
      "Epoch 63/100\n",
      "146/146 [==============================] - 16s - loss: 1.5134 - acc: 0.5045 - val_loss: 3.6765 - val_acc: 0.4062\n",
      "Epoch 64/100\n",
      "146/146 [==============================] - 16s - loss: 1.4850 - acc: 0.5075 - val_loss: 3.7855 - val_acc: 0.3931\n",
      "Epoch 65/100\n",
      "146/146 [==============================] - 16s - loss: 1.4844 - acc: 0.5106 - val_loss: 4.0079 - val_acc: 0.3900\n",
      "Epoch 66/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146/146 [==============================] - 16s - loss: 1.5066 - acc: 0.5115 - val_loss: 3.6983 - val_acc: 0.3931\n",
      "Epoch 67/100\n",
      "146/146 [==============================] - 16s - loss: 1.4672 - acc: 0.5151 - val_loss: 3.5599 - val_acc: 0.4163\n",
      "Epoch 68/100\n",
      "146/146 [==============================] - 16s - loss: 1.4766 - acc: 0.5179 - val_loss: 3.8117 - val_acc: 0.3997\n",
      "Epoch 69/100\n",
      "146/146 [==============================] - 16s - loss: 1.4733 - acc: 0.5068 - val_loss: 3.4648 - val_acc: 0.4053\n",
      "Epoch 70/100\n",
      "146/146 [==============================] - 16s - loss: 1.4894 - acc: 0.5104 - val_loss: 3.4112 - val_acc: 0.4237\n",
      "Epoch 71/100\n",
      "146/146 [==============================] - 16s - loss: 1.4615 - acc: 0.5233 - val_loss: 3.4647 - val_acc: 0.4110\n",
      "Epoch 72/100\n",
      "146/146 [==============================] - 16s - loss: 1.4795 - acc: 0.5137 - val_loss: 3.8134 - val_acc: 0.3843\n",
      "Epoch 73/100\n",
      "146/146 [==============================] - 16s - loss: 1.4461 - acc: 0.5208 - val_loss: 3.5803 - val_acc: 0.4036\n",
      "Epoch 74/100\n",
      "146/146 [==============================] - 16s - loss: 1.4686 - acc: 0.5134 - val_loss: 3.6718 - val_acc: 0.3992\n",
      "Epoch 75/100\n",
      "146/146 [==============================] - 16s - loss: 1.4700 - acc: 0.5144 - val_loss: 3.7124 - val_acc: 0.3950\n",
      "Epoch 76/100\n",
      "146/146 [==============================] - 16s - loss: 1.4519 - acc: 0.5172 - val_loss: 3.6114 - val_acc: 0.4106\n",
      "Epoch 77/100\n",
      "146/146 [==============================] - 16s - loss: 1.4459 - acc: 0.5205 - val_loss: 3.3605 - val_acc: 0.4106\n",
      "Epoch 78/100\n",
      "146/146 [==============================] - 16s - loss: 1.4630 - acc: 0.5189 - val_loss: 3.8223 - val_acc: 0.3822\n",
      "Epoch 79/100\n",
      "146/146 [==============================] - 16s - loss: 1.4465 - acc: 0.5215 - val_loss: 3.4068 - val_acc: 0.4123\n",
      "Epoch 80/100\n",
      "146/146 [==============================] - 16s - loss: 1.4243 - acc: 0.5302 - val_loss: 3.5555 - val_acc: 0.4071\n",
      "Epoch 81/100\n",
      "146/146 [==============================] - 16s - loss: 1.4324 - acc: 0.5236 - val_loss: 3.6196 - val_acc: 0.4058\n",
      "Epoch 82/100\n",
      "146/146 [==============================] - 16s - loss: 1.4378 - acc: 0.5258 - val_loss: 3.6645 - val_acc: 0.3962\n",
      "Epoch 83/100\n",
      "146/146 [==============================] - 16s - loss: 1.4205 - acc: 0.5324 - val_loss: 3.4668 - val_acc: 0.4136\n",
      "Epoch 84/100\n",
      "146/146 [==============================] - 16s - loss: 1.4199 - acc: 0.5341 - val_loss: 3.7159 - val_acc: 0.3979\n",
      "Epoch 85/100\n",
      "146/146 [==============================] - 16s - loss: 1.4383 - acc: 0.5267 - val_loss: 3.4131 - val_acc: 0.4228\n",
      "Epoch 86/100\n",
      "146/146 [==============================] - 16s - loss: 1.4179 - acc: 0.5312 - val_loss: 3.7012 - val_acc: 0.4097\n",
      "Epoch 87/100\n",
      "146/146 [==============================] - 16s - loss: 1.4236 - acc: 0.5345 - val_loss: 3.5646 - val_acc: 0.4211\n",
      "Epoch 88/100\n",
      "146/146 [==============================] - 16s - loss: 1.4259 - acc: 0.5276 - val_loss: 3.0292 - val_acc: 0.4228\n",
      "Epoch 89/100\n",
      "146/146 [==============================] - 16s - loss: 1.4342 - acc: 0.5311 - val_loss: 3.6801 - val_acc: 0.3900\n",
      "Epoch 90/100\n",
      "146/146 [==============================] - 16s - loss: 1.4026 - acc: 0.5355 - val_loss: 3.6671 - val_acc: 0.4036\n",
      "Epoch 91/100\n",
      "146/146 [==============================] - 16s - loss: 1.4086 - acc: 0.5392 - val_loss: 3.5571 - val_acc: 0.4075\n",
      "Epoch 92/100\n",
      "146/146 [==============================] - 16s - loss: 1.4273 - acc: 0.5370 - val_loss: 3.4881 - val_acc: 0.4167\n",
      "Epoch 93/100\n",
      "146/146 [==============================] - 16s - loss: 1.4058 - acc: 0.5390 - val_loss: 3.3870 - val_acc: 0.4198\n",
      "Epoch 94/100\n",
      "146/146 [==============================] - 16s - loss: 1.4212 - acc: 0.5297 - val_loss: 3.3355 - val_acc: 0.4206\n",
      "Epoch 95/100\n",
      "146/146 [==============================] - 16s - loss: 1.4148 - acc: 0.5329 - val_loss: 3.4441 - val_acc: 0.4066\n",
      "Epoch 96/100\n",
      "146/146 [==============================] - 16s - loss: 1.3826 - acc: 0.5482 - val_loss: 3.2898 - val_acc: 0.4276\n",
      "Epoch 97/100\n",
      "146/146 [==============================] - 16s - loss: 1.3984 - acc: 0.5389 - val_loss: 3.4804 - val_acc: 0.4215\n",
      "Epoch 98/100\n",
      "146/146 [==============================] - 16s - loss: 1.3980 - acc: 0.5441 - val_loss: 3.3160 - val_acc: 0.4171\n",
      "Epoch 99/100\n",
      "146/146 [==============================] - 16s - loss: 1.4050 - acc: 0.5369 - val_loss: 3.5190 - val_acc: 0.4058\n",
      "Epoch 100/100\n",
      "146/146 [==============================] - 16s - loss: 1.3995 - acc: 0.5404 - val_loss: 3.2923 - val_acc: 0.4202\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1422343fd0>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn8.fit_generator(train_generator3, steps_per_epoch=X_train.shape[0] // 64, epochs=100, \n",
    "                    validation_data=test_generator3, validation_steps=len(y_test_1hot)//64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "50 100 0.15 0.25 0.25 32 lr=0.4 150 =0.7 \n",
    "    \n",
    " 70 100 0.1 0.2 0.2 32 lr=0.4 150     =0.7\n",
    "\n",
    " 70 100 0.1 0.2 0.2 128 lr= 0.4 150  = 0.71\n",
    " \n",
    " 70 100 xx xx xx  128 lr=0.4    150  =0.71 \n",
    " \n",
    " 30 60 xx xx xx  128 lr= 0.3   200  =0.74\n",
    " \n",
    "''' \n",
    " \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#try4________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn18 = Sequential([\n",
    "    Conv2D(30, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=(100,100,3)),\n",
    "#     Dropout(.1),\n",
    "    MaxPool2D((2,2)),\n",
    "    \n",
    "    Conv2D(60, (3, 3), activation='relu'),\n",
    "#     BatchNormalization(axis=-1),\n",
    "#     Dropout(.2),\n",
    "    MaxPool2D((2,2)),\n",
    "    \n",
    "#     Conv2D(60, (3, 3), activation='relu'),\n",
    "#     BatchNormalization(axis=-1),\n",
    "#     Dropout(.25),\n",
    "#     MaxPool2D((2,2)),\n",
    "    \n",
    "#     Conv2D(80, (3, 3), activation='relu'),\n",
    "#     BatchNormalization(axis=-1),\n",
    "#     Dropout(.25),\n",
    "#     MaxPool2D((2,2)),\n",
    "    \n",
    "#     Conv2D(100, (3, 3), activation='relu'),\n",
    "#     Dropout(.5),\n",
    "#     MaxPool2D((2,2)),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "#     BatchNormalization(axis=-1),\n",
    "#     Dropout(.2),\n",
    "    Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_54 (Conv2D)           (None, 98, 98, 30)        840       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_54 (MaxPooling (None, 49, 49, 30)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_55 (Conv2D)           (None, 47, 47, 60)        16260     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_55 (MaxPooling (None, 23, 23, 60)        0         \n",
      "_________________________________________________________________\n",
      "flatten_22 (Flatten)         (None, 31740)             0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 128)               4062848   \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 4,081,238\n",
      "Trainable params: 4,081,238\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn18.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn18.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(lr=0.3),\n",
    "#               optimizer=Adam(lr=0.2),\n",
    "#               optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imgen_train2 = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=90,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    ")\n",
    "\n",
    "imgen_test2=ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_generator4 = imgen_train2.flow(X_train, y_train_1hot, batch_size=64)\n",
    "test_generator4 = imgen_test2.flow(X_test, y_test_1hot, batch_size=64)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "146/146 [==============================] - 17s - loss: 1.8002 - acc: 0.3592 - val_loss: 1.6338 - val_acc: 0.4562\n",
      "Epoch 2/200\n",
      "146/146 [==============================] - 16s - loss: 1.5500 - acc: 0.4639 - val_loss: 1.4442 - val_acc: 0.5146\n",
      "Epoch 3/200\n",
      "146/146 [==============================] - 16s - loss: 1.4300 - acc: 0.5114 - val_loss: 1.4341 - val_acc: 0.5247\n",
      "Epoch 4/200\n",
      "146/146 [==============================] - 16s - loss: 1.3757 - acc: 0.5352 - val_loss: 1.3412 - val_acc: 0.5588\n",
      "Epoch 5/200\n",
      "146/146 [==============================] - 16s - loss: 1.2983 - acc: 0.5634 - val_loss: 1.2718 - val_acc: 0.5960\n",
      "Epoch 6/200\n",
      "146/146 [==============================] - 16s - loss: 1.2428 - acc: 0.5839 - val_loss: 1.3131 - val_acc: 0.5697\n",
      "Epoch 7/200\n",
      "146/146 [==============================] - 16s - loss: 1.2529 - acc: 0.5750 - val_loss: 1.2301 - val_acc: 0.5903\n",
      "Epoch 8/200\n",
      "146/146 [==============================] - 16s - loss: 1.1863 - acc: 0.6018 - val_loss: 1.2245 - val_acc: 0.6030\n",
      "Epoch 9/200\n",
      "146/146 [==============================] - 16s - loss: 1.1858 - acc: 0.6018 - val_loss: 1.1819 - val_acc: 0.6060\n",
      "Epoch 10/200\n",
      "146/146 [==============================] - 16s - loss: 1.1698 - acc: 0.6033 - val_loss: 1.1813 - val_acc: 0.6043\n",
      "Epoch 11/200\n",
      "146/146 [==============================] - 16s - loss: 1.1349 - acc: 0.6168 - val_loss: 1.1621 - val_acc: 0.6240\n",
      "Epoch 12/200\n",
      "146/146 [==============================] - 16s - loss: 1.1572 - acc: 0.6111 - val_loss: 1.1104 - val_acc: 0.6458\n",
      "Epoch 13/200\n",
      "146/146 [==============================] - 16s - loss: 1.1098 - acc: 0.6227 - val_loss: 1.1320 - val_acc: 0.6288\n",
      "Epoch 14/200\n",
      "146/146 [==============================] - 16s - loss: 1.1123 - acc: 0.6240 - val_loss: 1.1194 - val_acc: 0.6423\n",
      "Epoch 15/200\n",
      "146/146 [==============================] - 16s - loss: 1.0882 - acc: 0.6373 - val_loss: 1.0799 - val_acc: 0.6611\n",
      "Epoch 16/200\n",
      "146/146 [==============================] - 16s - loss: 1.0812 - acc: 0.6307 - val_loss: 1.0701 - val_acc: 0.6550\n",
      "Epoch 17/200\n",
      "146/146 [==============================] - 16s - loss: 1.0684 - acc: 0.6419 - val_loss: 1.0799 - val_acc: 0.6393\n",
      "Epoch 18/200\n",
      "146/146 [==============================] - 16s - loss: 1.0786 - acc: 0.6429 - val_loss: 1.0913 - val_acc: 0.6340\n",
      "Epoch 19/200\n",
      "146/146 [==============================] - 16s - loss: 1.0599 - acc: 0.6401 - val_loss: 1.0286 - val_acc: 0.6603\n",
      "Epoch 20/200\n",
      "146/146 [==============================] - 16s - loss: 1.0400 - acc: 0.6507 - val_loss: 1.1041 - val_acc: 0.6366\n",
      "Epoch 21/200\n",
      "146/146 [==============================] - 16s - loss: 1.0439 - acc: 0.6458 - val_loss: 1.0301 - val_acc: 0.6581\n",
      "Epoch 22/200\n",
      "146/146 [==============================] - 16s - loss: 1.0306 - acc: 0.6544 - val_loss: 1.0590 - val_acc: 0.6576\n",
      "Epoch 23/200\n",
      "146/146 [==============================] - 16s - loss: 1.0333 - acc: 0.6554 - val_loss: 1.0285 - val_acc: 0.6729\n",
      "Epoch 24/200\n",
      "146/146 [==============================] - 16s - loss: 1.0187 - acc: 0.6570 - val_loss: 1.0545 - val_acc: 0.6546\n",
      "Epoch 25/200\n",
      "146/146 [==============================] - 16s - loss: 1.0104 - acc: 0.6638 - val_loss: 1.0156 - val_acc: 0.6738\n",
      "Epoch 26/200\n",
      "146/146 [==============================] - 16s - loss: 1.0115 - acc: 0.6635 - val_loss: 1.0153 - val_acc: 0.6690\n",
      "Epoch 27/200\n",
      "146/146 [==============================] - 16s - loss: 1.0089 - acc: 0.6604 - val_loss: 1.0096 - val_acc: 0.6839\n",
      "Epoch 28/200\n",
      "146/146 [==============================] - 16s - loss: 0.9975 - acc: 0.6634 - val_loss: 0.9973 - val_acc: 0.6734\n",
      "Epoch 29/200\n",
      "146/146 [==============================] - 16s - loss: 0.9972 - acc: 0.6701 - val_loss: 1.0158 - val_acc: 0.6760\n",
      "Epoch 30/200\n",
      "146/146 [==============================] - 16s - loss: 0.9804 - acc: 0.6733 - val_loss: 1.0188 - val_acc: 0.6624\n",
      "Epoch 31/200\n",
      "146/146 [==============================] - 16s - loss: 0.9816 - acc: 0.6655 - val_loss: 0.9952 - val_acc: 0.6751\n",
      "Epoch 32/200\n",
      "146/146 [==============================] - 16s - loss: 0.9693 - acc: 0.6762 - val_loss: 1.0064 - val_acc: 0.6672\n",
      "Epoch 33/200\n",
      "146/146 [==============================] - 16s - loss: 0.9679 - acc: 0.6757 - val_loss: 0.9851 - val_acc: 0.6764\n",
      "Epoch 34/200\n",
      "146/146 [==============================] - 16s - loss: 0.9617 - acc: 0.6746 - val_loss: 0.9964 - val_acc: 0.6808\n",
      "Epoch 35/200\n",
      "146/146 [==============================] - 16s - loss: 0.9649 - acc: 0.6796 - val_loss: 1.0175 - val_acc: 0.6686\n",
      "Epoch 36/200\n",
      "146/146 [==============================] - 16s - loss: 0.9485 - acc: 0.6829 - val_loss: 0.9523 - val_acc: 0.6891\n",
      "Epoch 37/200\n",
      "146/146 [==============================] - 16s - loss: 0.9509 - acc: 0.6820 - val_loss: 0.9923 - val_acc: 0.6795\n",
      "Epoch 38/200\n",
      "146/146 [==============================] - 16s - loss: 0.9509 - acc: 0.6902 - val_loss: 0.9988 - val_acc: 0.6753\n",
      "Epoch 39/200\n",
      "146/146 [==============================] - 16s - loss: 0.9379 - acc: 0.6880 - val_loss: 0.9686 - val_acc: 0.6742\n",
      "Epoch 40/200\n",
      "146/146 [==============================] - 16s - loss: 0.9408 - acc: 0.6900 - val_loss: 0.9560 - val_acc: 0.6917\n",
      "Epoch 41/200\n",
      "146/146 [==============================] - 16s - loss: 0.9293 - acc: 0.6886 - val_loss: 0.9688 - val_acc: 0.6686\n",
      "Epoch 42/200\n",
      "146/146 [==============================] - 16s - loss: 0.9333 - acc: 0.6895 - val_loss: 0.9620 - val_acc: 0.6869\n",
      "Epoch 43/200\n",
      "146/146 [==============================] - 16s - loss: 0.9283 - acc: 0.6943 - val_loss: 1.0059 - val_acc: 0.6738\n",
      "Epoch 44/200\n",
      "146/146 [==============================] - 16s - loss: 0.9220 - acc: 0.6966 - val_loss: 0.9632 - val_acc: 0.6913\n",
      "Epoch 45/200\n",
      "146/146 [==============================] - 16s - loss: 0.9171 - acc: 0.6983 - val_loss: 0.9710 - val_acc: 0.6725\n",
      "Epoch 46/200\n",
      "146/146 [==============================] - 16s - loss: 0.9258 - acc: 0.6939 - val_loss: 0.9799 - val_acc: 0.6795\n",
      "Epoch 47/200\n",
      "146/146 [==============================] - 16s - loss: 0.8993 - acc: 0.7057 - val_loss: 0.9775 - val_acc: 0.6799\n",
      "Epoch 48/200\n",
      "146/146 [==============================] - 16s - loss: 0.9100 - acc: 0.6996 - val_loss: 0.9614 - val_acc: 0.6852\n",
      "Epoch 49/200\n",
      "146/146 [==============================] - 16s - loss: 0.8944 - acc: 0.7002 - val_loss: 0.9641 - val_acc: 0.6930\n",
      "Epoch 50/200\n",
      "146/146 [==============================] - 16s - loss: 0.9140 - acc: 0.6955 - val_loss: 0.9330 - val_acc: 0.6926\n",
      "Epoch 51/200\n",
      "146/146 [==============================] - 16s - loss: 0.8942 - acc: 0.7029 - val_loss: 0.9376 - val_acc: 0.6891\n",
      "Epoch 52/200\n",
      "146/146 [==============================] - 16s - loss: 0.8881 - acc: 0.7044 - val_loss: 0.9118 - val_acc: 0.6983\n",
      "Epoch 53/200\n",
      "146/146 [==============================] - 16s - loss: 0.9009 - acc: 0.6999 - val_loss: 0.9343 - val_acc: 0.6992\n",
      "Epoch 54/200\n",
      "146/146 [==============================] - 16s - loss: 0.8879 - acc: 0.7090 - val_loss: 0.9592 - val_acc: 0.6891\n",
      "Epoch 55/200\n",
      "146/146 [==============================] - 16s - loss: 0.8797 - acc: 0.7047 - val_loss: 0.9707 - val_acc: 0.6878\n",
      "Epoch 56/200\n",
      "146/146 [==============================] - 16s - loss: 0.8707 - acc: 0.7101 - val_loss: 0.9396 - val_acc: 0.6987\n",
      "Epoch 57/200\n",
      "146/146 [==============================] - 16s - loss: 0.8791 - acc: 0.7063 - val_loss: 0.9295 - val_acc: 0.6847\n",
      "Epoch 58/200\n",
      "146/146 [==============================] - 16s - loss: 0.8753 - acc: 0.7040 - val_loss: 0.9493 - val_acc: 0.6948\n",
      "Epoch 59/200\n",
      "146/146 [==============================] - 16s - loss: 0.8676 - acc: 0.7085 - val_loss: 0.9151 - val_acc: 0.6948\n",
      "Epoch 60/200\n",
      "146/146 [==============================] - 16s - loss: 0.8713 - acc: 0.7132 - val_loss: 0.9656 - val_acc: 0.6948\n",
      "Epoch 61/200\n",
      "146/146 [==============================] - 16s - loss: 0.8527 - acc: 0.7214 - val_loss: 0.9074 - val_acc: 0.7088\n",
      "Epoch 62/200\n",
      "146/146 [==============================] - 16s - loss: 0.8597 - acc: 0.7153 - val_loss: 0.9238 - val_acc: 0.6913\n",
      "Epoch 63/200\n",
      "146/146 [==============================] - 16s - loss: 0.8604 - acc: 0.7145 - val_loss: 0.9169 - val_acc: 0.7018\n",
      "Epoch 64/200\n",
      "146/146 [==============================] - 16s - loss: 0.8592 - acc: 0.7179 - val_loss: 0.9373 - val_acc: 0.6935\n",
      "Epoch 65/200\n",
      "146/146 [==============================] - 16s - loss: 0.8417 - acc: 0.7198 - val_loss: 0.9200 - val_acc: 0.7027\n",
      "Epoch 66/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146/146 [==============================] - 16s - loss: 0.8621 - acc: 0.7148 - val_loss: 0.9442 - val_acc: 0.6970\n",
      "Epoch 67/200\n",
      "146/146 [==============================] - 16s - loss: 0.8411 - acc: 0.7243 - val_loss: 0.9304 - val_acc: 0.6957\n",
      "Epoch 68/200\n",
      "146/146 [==============================] - 16s - loss: 0.8517 - acc: 0.7195 - val_loss: 0.8845 - val_acc: 0.7153\n",
      "Epoch 69/200\n",
      "146/146 [==============================] - 16s - loss: 0.8256 - acc: 0.7285 - val_loss: 0.9106 - val_acc: 0.7040\n",
      "Epoch 70/200\n",
      "146/146 [==============================] - 16s - loss: 0.8365 - acc: 0.7240 - val_loss: 0.9698 - val_acc: 0.6882\n",
      "Epoch 71/200\n",
      "146/146 [==============================] - 16s - loss: 0.8320 - acc: 0.7248 - val_loss: 0.8926 - val_acc: 0.7101\n",
      "Epoch 72/200\n",
      "146/146 [==============================] - 16s - loss: 0.8443 - acc: 0.7196 - val_loss: 0.9039 - val_acc: 0.7088\n",
      "Epoch 73/200\n",
      "146/146 [==============================] - 16s - loss: 0.8294 - acc: 0.7250 - val_loss: 0.9174 - val_acc: 0.7014\n",
      "Epoch 74/200\n",
      "146/146 [==============================] - 16s - loss: 0.8339 - acc: 0.7221 - val_loss: 0.9116 - val_acc: 0.7014\n",
      "Epoch 75/200\n",
      "146/146 [==============================] - 16s - loss: 0.8253 - acc: 0.7248 - val_loss: 0.9113 - val_acc: 0.7049\n",
      "Epoch 76/200\n",
      "146/146 [==============================] - 16s - loss: 0.8289 - acc: 0.7270 - val_loss: 0.9419 - val_acc: 0.6887\n",
      "Epoch 77/200\n",
      "146/146 [==============================] - 16s - loss: 0.8064 - acc: 0.7309 - val_loss: 0.8562 - val_acc: 0.7210\n",
      "Epoch 78/200\n",
      "146/146 [==============================] - 16s - loss: 0.8149 - acc: 0.7295 - val_loss: 0.8840 - val_acc: 0.7210\n",
      "Epoch 79/200\n",
      "146/146 [==============================] - 16s - loss: 0.8097 - acc: 0.7273 - val_loss: 0.9270 - val_acc: 0.6979\n",
      "Epoch 80/200\n",
      "146/146 [==============================] - 16s - loss: 0.8149 - acc: 0.7364 - val_loss: 0.8949 - val_acc: 0.7158\n",
      "Epoch 81/200\n",
      "146/146 [==============================] - 16s - loss: 0.8076 - acc: 0.7311 - val_loss: 0.8739 - val_acc: 0.7188\n",
      "Epoch 82/200\n",
      "146/146 [==============================] - 16s - loss: 0.8091 - acc: 0.7359 - val_loss: 0.9012 - val_acc: 0.7158\n",
      "Epoch 83/200\n",
      "146/146 [==============================] - 16s - loss: 0.7987 - acc: 0.7350 - val_loss: 0.8778 - val_acc: 0.7202\n",
      "Epoch 84/200\n",
      "146/146 [==============================] - 16s - loss: 0.8134 - acc: 0.7313 - val_loss: 0.8875 - val_acc: 0.7149\n",
      "Epoch 85/200\n",
      "146/146 [==============================] - 16s - loss: 0.7955 - acc: 0.7360 - val_loss: 0.9407 - val_acc: 0.6874\n",
      "Epoch 86/200\n",
      "146/146 [==============================] - 16s - loss: 0.8005 - acc: 0.7338 - val_loss: 0.9049 - val_acc: 0.7162\n",
      "Epoch 87/200\n",
      "146/146 [==============================] - 16s - loss: 0.8014 - acc: 0.7358 - val_loss: 0.8808 - val_acc: 0.7127\n",
      "Epoch 88/200\n",
      "146/146 [==============================] - 16s - loss: 0.7891 - acc: 0.7362 - val_loss: 0.8421 - val_acc: 0.7149\n",
      "Epoch 89/200\n",
      "146/146 [==============================] - 16s - loss: 0.7922 - acc: 0.7391 - val_loss: 0.9179 - val_acc: 0.6939\n",
      "Epoch 90/200\n",
      "146/146 [==============================] - 16s - loss: 0.8017 - acc: 0.7311 - val_loss: 0.8906 - val_acc: 0.7114\n",
      "Epoch 91/200\n",
      "146/146 [==============================] - 16s - loss: 0.7842 - acc: 0.7373 - val_loss: 0.8971 - val_acc: 0.7092\n",
      "Epoch 92/200\n",
      "146/146 [==============================] - 16s - loss: 0.7877 - acc: 0.7407 - val_loss: 0.8860 - val_acc: 0.7049\n",
      "Epoch 93/200\n",
      "146/146 [==============================] - 16s - loss: 0.7874 - acc: 0.7412 - val_loss: 0.8778 - val_acc: 0.7167\n",
      "Epoch 94/200\n",
      "146/146 [==============================] - 16s - loss: 0.7754 - acc: 0.7449 - val_loss: 0.8712 - val_acc: 0.7084\n",
      "Epoch 95/200\n",
      "146/146 [==============================] - 16s - loss: 0.7777 - acc: 0.7431 - val_loss: 0.9290 - val_acc: 0.7057\n",
      "Epoch 96/200\n",
      "146/146 [==============================] - 16s - loss: 0.7765 - acc: 0.7465 - val_loss: 0.9053 - val_acc: 0.7005\n",
      "Epoch 97/200\n",
      "146/146 [==============================] - 16s - loss: 0.7652 - acc: 0.7486 - val_loss: 0.8922 - val_acc: 0.7202\n",
      "Epoch 98/200\n",
      "146/146 [==============================] - 16s - loss: 0.7630 - acc: 0.7512 - val_loss: 0.8754 - val_acc: 0.7101\n",
      "Epoch 99/200\n",
      "146/146 [==============================] - 16s - loss: 0.7768 - acc: 0.7412 - val_loss: 0.8989 - val_acc: 0.7088\n",
      "Epoch 100/200\n",
      "146/146 [==============================] - 16s - loss: 0.7643 - acc: 0.7491 - val_loss: 0.8904 - val_acc: 0.7149\n",
      "Epoch 101/200\n",
      "146/146 [==============================] - 16s - loss: 0.7618 - acc: 0.7473 - val_loss: 0.8819 - val_acc: 0.7097\n",
      "Epoch 102/200\n",
      "146/146 [==============================] - 16s - loss: 0.7686 - acc: 0.7455 - val_loss: 0.9028 - val_acc: 0.7097\n",
      "Epoch 103/200\n",
      "146/146 [==============================] - 16s - loss: 0.7575 - acc: 0.7512 - val_loss: 0.8621 - val_acc: 0.7202\n",
      "Epoch 104/200\n",
      "146/146 [==============================] - 16s - loss: 0.7830 - acc: 0.7400 - val_loss: 0.8523 - val_acc: 0.7219\n",
      "Epoch 105/200\n",
      "146/146 [==============================] - 16s - loss: 0.7429 - acc: 0.7578 - val_loss: 0.8994 - val_acc: 0.7114\n",
      "Epoch 106/200\n",
      "146/146 [==============================] - 16s - loss: 0.7548 - acc: 0.7516 - val_loss: 0.8405 - val_acc: 0.7237\n",
      "Epoch 107/200\n",
      "146/146 [==============================] - 16s - loss: 0.7704 - acc: 0.7427 - val_loss: 0.8947 - val_acc: 0.7123\n",
      "Epoch 108/200\n",
      "146/146 [==============================] - 16s - loss: 0.7426 - acc: 0.7532 - val_loss: 0.8735 - val_acc: 0.7184\n",
      "Epoch 109/200\n",
      "146/146 [==============================] - 16s - loss: 0.7583 - acc: 0.7529 - val_loss: 0.8535 - val_acc: 0.7250\n",
      "Epoch 110/200\n",
      "146/146 [==============================] - 17s - loss: 0.7545 - acc: 0.7472 - val_loss: 0.8314 - val_acc: 0.7328\n",
      "Epoch 111/200\n",
      "146/146 [==============================] - 16s - loss: 0.7388 - acc: 0.7559 - val_loss: 0.8830 - val_acc: 0.7140\n",
      "Epoch 112/200\n",
      "146/146 [==============================] - 16s - loss: 0.7408 - acc: 0.7582 - val_loss: 0.8768 - val_acc: 0.7131\n",
      "Epoch 113/200\n",
      "146/146 [==============================] - 16s - loss: 0.7541 - acc: 0.7451 - val_loss: 0.8860 - val_acc: 0.7228\n",
      "Epoch 114/200\n",
      "146/146 [==============================] - 16s - loss: 0.7538 - acc: 0.7508 - val_loss: 0.9236 - val_acc: 0.7000\n",
      "Epoch 115/200\n",
      "146/146 [==============================] - 16s - loss: 0.7396 - acc: 0.7556 - val_loss: 0.8672 - val_acc: 0.7110\n",
      "Epoch 116/200\n",
      "146/146 [==============================] - 16s - loss: 0.7426 - acc: 0.7527 - val_loss: 0.8868 - val_acc: 0.7084\n",
      "Epoch 117/200\n",
      "146/146 [==============================] - 16s - loss: 0.7374 - acc: 0.7545 - val_loss: 0.8757 - val_acc: 0.7241\n",
      "Epoch 118/200\n",
      "146/146 [==============================] - 16s - loss: 0.7245 - acc: 0.7575 - val_loss: 0.8503 - val_acc: 0.7223\n",
      "Epoch 119/200\n",
      "146/146 [==============================] - 16s - loss: 0.7409 - acc: 0.7564 - val_loss: 0.8462 - val_acc: 0.7254\n",
      "Epoch 120/200\n",
      "146/146 [==============================] - 17s - loss: 0.7402 - acc: 0.7570 - val_loss: 0.8631 - val_acc: 0.7180\n",
      "Epoch 121/200\n",
      "146/146 [==============================] - 16s - loss: 0.7389 - acc: 0.7566 - val_loss: 0.9145 - val_acc: 0.7018\n",
      "Epoch 122/200\n",
      "146/146 [==============================] - 16s - loss: 0.7350 - acc: 0.7573 - val_loss: 0.8751 - val_acc: 0.7114\n",
      "Epoch 123/200\n",
      "146/146 [==============================] - 16s - loss: 0.7285 - acc: 0.7581 - val_loss: 0.8608 - val_acc: 0.7202\n",
      "Epoch 124/200\n",
      "146/146 [==============================] - 16s - loss: 0.7286 - acc: 0.7570 - val_loss: 0.8678 - val_acc: 0.7223\n",
      "Epoch 125/200\n",
      "146/146 [==============================] - 16s - loss: 0.7185 - acc: 0.7623 - val_loss: 0.8581 - val_acc: 0.7219\n",
      "Epoch 126/200\n",
      "146/146 [==============================] - 16s - loss: 0.7218 - acc: 0.7629 - val_loss: 0.8610 - val_acc: 0.7237\n",
      "Epoch 127/200\n",
      "146/146 [==============================] - 16s - loss: 0.7240 - acc: 0.7578 - val_loss: 0.8922 - val_acc: 0.7285\n",
      "Epoch 128/200\n",
      "146/146 [==============================] - 16s - loss: 0.7108 - acc: 0.7651 - val_loss: 0.8590 - val_acc: 0.7298\n",
      "Epoch 129/200\n",
      "146/146 [==============================] - 16s - loss: 0.7353 - acc: 0.7589 - val_loss: 0.8479 - val_acc: 0.7223\n",
      "Epoch 130/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146/146 [==============================] - 16s - loss: 0.7033 - acc: 0.7673 - val_loss: 0.8730 - val_acc: 0.7241\n",
      "Epoch 131/200\n",
      "146/146 [==============================] - 16s - loss: 0.7190 - acc: 0.7616 - val_loss: 0.8748 - val_acc: 0.7202\n",
      "Epoch 132/200\n",
      "146/146 [==============================] - 16s - loss: 0.7067 - acc: 0.7641 - val_loss: 0.8670 - val_acc: 0.7202\n",
      "Epoch 133/200\n",
      "146/146 [==============================] - 16s - loss: 0.7161 - acc: 0.7638 - val_loss: 0.8616 - val_acc: 0.7149\n",
      "Epoch 134/200\n",
      "146/146 [==============================] - 16s - loss: 0.7207 - acc: 0.7605 - val_loss: 0.9342 - val_acc: 0.6970\n",
      "Epoch 135/200\n",
      "146/146 [==============================] - 16s - loss: 0.7008 - acc: 0.7682 - val_loss: 0.8409 - val_acc: 0.7193\n",
      "Epoch 136/200\n",
      "146/146 [==============================] - 16s - loss: 0.7011 - acc: 0.7669 - val_loss: 0.8717 - val_acc: 0.7215\n",
      "Epoch 137/200\n",
      "146/146 [==============================] - 16s - loss: 0.7112 - acc: 0.7625 - val_loss: 0.8497 - val_acc: 0.7267\n",
      "Epoch 138/200\n",
      "146/146 [==============================] - 16s - loss: 0.7173 - acc: 0.7664 - val_loss: 0.8237 - val_acc: 0.7285\n",
      "Epoch 139/200\n",
      "146/146 [==============================] - 16s - loss: 0.7105 - acc: 0.7626 - val_loss: 0.8302 - val_acc: 0.7376\n",
      "Epoch 140/200\n",
      "146/146 [==============================] - 16s - loss: 0.6907 - acc: 0.7696 - val_loss: 0.8666 - val_acc: 0.7175\n",
      "Epoch 141/200\n",
      "146/146 [==============================] - 16s - loss: 0.6985 - acc: 0.7705 - val_loss: 0.8951 - val_acc: 0.7057\n",
      "Epoch 142/200\n",
      "146/146 [==============================] - 16s - loss: 0.7022 - acc: 0.7662 - val_loss: 0.8366 - val_acc: 0.7390\n",
      "Epoch 143/200\n",
      "146/146 [==============================] - 16s - loss: 0.7010 - acc: 0.7671 - val_loss: 0.8618 - val_acc: 0.7193\n",
      "Epoch 144/200\n",
      "146/146 [==============================] - 16s - loss: 0.6980 - acc: 0.7678 - val_loss: 0.8784 - val_acc: 0.7223\n",
      "Epoch 145/200\n",
      "146/146 [==============================] - 16s - loss: 0.6969 - acc: 0.7680 - val_loss: 0.8366 - val_acc: 0.7320\n",
      "Epoch 146/200\n",
      "146/146 [==============================] - 16s - loss: 0.7001 - acc: 0.7675 - val_loss: 0.8801 - val_acc: 0.7158\n",
      "Epoch 147/200\n",
      "146/146 [==============================] - 16s - loss: 0.7095 - acc: 0.7615 - val_loss: 0.8403 - val_acc: 0.7267\n",
      "Epoch 148/200\n",
      "146/146 [==============================] - 16s - loss: 0.6992 - acc: 0.7660 - val_loss: 0.8561 - val_acc: 0.7263\n",
      "Epoch 149/200\n",
      "146/146 [==============================] - 16s - loss: 0.6898 - acc: 0.7737 - val_loss: 0.8385 - val_acc: 0.7378\n",
      "Epoch 150/200\n",
      "146/146 [==============================] - 16s - loss: 0.6887 - acc: 0.7723 - val_loss: 0.8107 - val_acc: 0.7398\n",
      "Epoch 151/200\n",
      "146/146 [==============================] - 16s - loss: 0.6954 - acc: 0.7699 - val_loss: 0.8491 - val_acc: 0.7272\n",
      "Epoch 152/200\n",
      "146/146 [==============================] - 16s - loss: 0.6910 - acc: 0.7699 - val_loss: 0.8499 - val_acc: 0.7258\n",
      "Epoch 153/200\n",
      "146/146 [==============================] - 16s - loss: 0.6871 - acc: 0.7686 - val_loss: 0.8607 - val_acc: 0.7193\n",
      "Epoch 154/200\n",
      "146/146 [==============================] - 16s - loss: 0.6960 - acc: 0.7668 - val_loss: 0.8538 - val_acc: 0.7263\n",
      "Epoch 155/200\n",
      "146/146 [==============================] - 16s - loss: 0.6852 - acc: 0.7697 - val_loss: 0.8230 - val_acc: 0.7302\n",
      "Epoch 156/200\n",
      "146/146 [==============================] - 16s - loss: 0.6781 - acc: 0.7782 - val_loss: 0.8578 - val_acc: 0.7289\n",
      "Epoch 157/200\n",
      "146/146 [==============================] - 16s - loss: 0.6819 - acc: 0.7731 - val_loss: 0.8381 - val_acc: 0.7425\n",
      "Epoch 158/200\n",
      "146/146 [==============================] - 16s - loss: 0.6889 - acc: 0.7712 - val_loss: 0.8225 - val_acc: 0.7385\n",
      "Epoch 159/200\n",
      "146/146 [==============================] - 16s - loss: 0.6768 - acc: 0.7694 - val_loss: 0.8886 - val_acc: 0.7105\n",
      "Epoch 160/200\n",
      "146/146 [==============================] - 16s - loss: 0.6779 - acc: 0.7720 - val_loss: 0.8470 - val_acc: 0.7372\n",
      "Epoch 161/200\n",
      "146/146 [==============================] - 16s - loss: 0.6840 - acc: 0.7716 - val_loss: 0.8389 - val_acc: 0.7293\n",
      "Epoch 162/200\n",
      "146/146 [==============================] - 16s - loss: 0.6821 - acc: 0.7731 - val_loss: 0.8384 - val_acc: 0.7341\n",
      "Epoch 163/200\n",
      "146/146 [==============================] - 16s - loss: 0.6762 - acc: 0.7800 - val_loss: 0.8570 - val_acc: 0.7328\n",
      "Epoch 164/200\n",
      "146/146 [==============================] - 16s - loss: 0.6851 - acc: 0.7704 - val_loss: 0.8529 - val_acc: 0.7289\n",
      "Epoch 165/200\n",
      "146/146 [==============================] - 16s - loss: 0.6709 - acc: 0.7766 - val_loss: 0.8530 - val_acc: 0.7232\n",
      "Epoch 166/200\n",
      "146/146 [==============================] - 16s - loss: 0.6723 - acc: 0.7774 - val_loss: 0.8519 - val_acc: 0.7267\n",
      "Epoch 167/200\n",
      "146/146 [==============================] - 16s - loss: 0.6713 - acc: 0.7785 - val_loss: 0.8673 - val_acc: 0.7180\n",
      "Epoch 168/200\n",
      "146/146 [==============================] - 16s - loss: 0.6759 - acc: 0.7725 - val_loss: 0.9001 - val_acc: 0.7180\n",
      "Epoch 169/200\n",
      "146/146 [==============================] - 16s - loss: 0.6659 - acc: 0.7788 - val_loss: 0.8764 - val_acc: 0.7197\n",
      "Epoch 170/200\n",
      "146/146 [==============================] - 16s - loss: 0.6668 - acc: 0.7756 - val_loss: 0.8477 - val_acc: 0.7355\n",
      "Epoch 171/200\n",
      "146/146 [==============================] - 16s - loss: 0.6633 - acc: 0.7812 - val_loss: 0.8697 - val_acc: 0.7298\n",
      "Epoch 172/200\n",
      "146/146 [==============================] - 16s - loss: 0.6653 - acc: 0.7790 - val_loss: 0.8489 - val_acc: 0.7258\n",
      "Epoch 173/200\n",
      "146/146 [==============================] - 16s - loss: 0.6690 - acc: 0.7760 - val_loss: 0.8150 - val_acc: 0.7372\n",
      "Epoch 174/200\n",
      "146/146 [==============================] - 16s - loss: 0.6627 - acc: 0.7799 - val_loss: 0.8353 - val_acc: 0.7363\n",
      "Epoch 175/200\n",
      "146/146 [==============================] - 16s - loss: 0.6657 - acc: 0.7797 - val_loss: 0.8491 - val_acc: 0.7267\n",
      "Epoch 176/200\n",
      "146/146 [==============================] - 16s - loss: 0.6542 - acc: 0.7832 - val_loss: 0.8576 - val_acc: 0.7267\n",
      "Epoch 177/200\n",
      "146/146 [==============================] - 16s - loss: 0.6644 - acc: 0.7792 - val_loss: 0.8560 - val_acc: 0.7263\n",
      "Epoch 178/200\n",
      "146/146 [==============================] - 16s - loss: 0.6615 - acc: 0.7841 - val_loss: 0.8454 - val_acc: 0.7280\n",
      "Epoch 179/200\n",
      "146/146 [==============================] - 16s - loss: 0.6526 - acc: 0.7849 - val_loss: 0.8355 - val_acc: 0.7337\n",
      "Epoch 180/200\n",
      "146/146 [==============================] - 16s - loss: 0.6660 - acc: 0.7828 - val_loss: 0.8465 - val_acc: 0.7302\n",
      "Epoch 181/200\n",
      "146/146 [==============================] - 16s - loss: 0.6610 - acc: 0.7744 - val_loss: 0.8833 - val_acc: 0.7237\n",
      "Epoch 182/200\n",
      "146/146 [==============================] - 16s - loss: 0.6559 - acc: 0.7841 - val_loss: 0.8243 - val_acc: 0.7359\n",
      "Epoch 183/200\n",
      "146/146 [==============================] - 16s - loss: 0.6560 - acc: 0.7811 - val_loss: 0.8154 - val_acc: 0.7429\n",
      "Epoch 184/200\n",
      "146/146 [==============================] - 16s - loss: 0.6488 - acc: 0.7856 - val_loss: 0.8630 - val_acc: 0.7355\n",
      "Epoch 185/200\n",
      "146/146 [==============================] - 16s - loss: 0.6554 - acc: 0.7845 - val_loss: 0.8473 - val_acc: 0.7118\n",
      "Epoch 186/200\n",
      "146/146 [==============================] - 16s - loss: 0.6590 - acc: 0.7839 - val_loss: 0.8267 - val_acc: 0.7296\n",
      "Epoch 187/200\n",
      "146/146 [==============================] - 16s - loss: 0.6506 - acc: 0.7823 - val_loss: 0.8449 - val_acc: 0.7307\n",
      "Epoch 188/200\n",
      "146/146 [==============================] - 16s - loss: 0.6488 - acc: 0.7906 - val_loss: 0.8224 - val_acc: 0.7368\n",
      "Epoch 189/200\n",
      "146/146 [==============================] - 16s - loss: 0.6523 - acc: 0.7823 - val_loss: 0.8677 - val_acc: 0.7237\n",
      "Epoch 190/200\n",
      "146/146 [==============================] - 16s - loss: 0.6414 - acc: 0.7896 - val_loss: 0.8744 - val_acc: 0.7228\n",
      "Epoch 191/200\n",
      "146/146 [==============================] - 16s - loss: 0.6535 - acc: 0.7857 - val_loss: 0.8368 - val_acc: 0.7425\n",
      "Epoch 192/200\n",
      "146/146 [==============================] - 16s - loss: 0.6429 - acc: 0.7846 - val_loss: 0.8678 - val_acc: 0.7250\n",
      "Epoch 193/200\n",
      "146/146 [==============================] - 16s - loss: 0.6425 - acc: 0.7842 - val_loss: 0.8679 - val_acc: 0.7280\n",
      "Epoch 194/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146/146 [==============================] - 16s - loss: 0.6477 - acc: 0.7821 - val_loss: 0.8229 - val_acc: 0.7403\n",
      "Epoch 195/200\n",
      "146/146 [==============================] - 16s - loss: 0.6415 - acc: 0.7857 - val_loss: 0.8316 - val_acc: 0.7350\n",
      "Epoch 196/200\n",
      "146/146 [==============================] - 16s - loss: 0.6386 - acc: 0.7870 - val_loss: 0.8456 - val_acc: 0.7272\n",
      "Epoch 197/200\n",
      "146/146 [==============================] - 16s - loss: 0.6356 - acc: 0.7892 - val_loss: 0.7875 - val_acc: 0.7455\n",
      "Epoch 198/200\n",
      "146/146 [==============================] - 16s - loss: 0.6411 - acc: 0.7895 - val_loss: 0.8324 - val_acc: 0.7390\n",
      "Epoch 199/200\n",
      "146/146 [==============================] - 16s - loss: 0.6343 - acc: 0.7900 - val_loss: 0.7903 - val_acc: 0.7499\n",
      "Epoch 200/200\n",
      "146/146 [==============================] - 16s - loss: 0.6347 - acc: 0.7916 - val_loss: 0.8105 - val_acc: 0.7298\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f13a3156650>"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn18.fit_generator(train_generator4, steps_per_epoch=X_train.shape[0] // 64, epochs=200, \n",
    "                    validation_data=test_generator4, validation_steps=len(y_test_1hot)//64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 72.65%\n"
     ]
    }
   ],
   "source": [
    "scores18 = cnn18.evaluate(X_test_rescale, y_test_1hot, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (cnn18.metrics_names[1], scores18[1]*100))\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "\n",
    "cnn18_json = cnn18.to_json()\n",
    "\n",
    "with open(\"cnn18.json\", \"w\") as json_file:\n",
    "    json_file.write(cnn18_json)\n",
    "    \n",
    "# serialize weights to HDF5\n",
    "cnn18.save_weights(\"cnn18.h5\")\n",
    "print(\"Saved model to disk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# load json and create model\n",
    "json_file = open('cnn18.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"cnn18.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 72.65%\n"
     ]
    }
   ],
   "source": [
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(lr=0.3),\n",
    "#               optimizer=Adam(lr=0.2),\n",
    "#               optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "score2 = loaded_model.evaluate(X_test_rescale, y_test_1hot, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score2[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# save the model to disk\n",
    "# pickle.dump(cnn18, open('cnn18.sav', 'wb'))\n",
    "\n",
    "\n",
    "# load the model from disk\n",
    "# loaded_model1 = pickle.load(open('cnn1.sav', 'rb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_list2=create_image_list(new_image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_image_list(image_dir):\n",
    "    img_dataset=[]\n",
    "    label_dataset=[]\n",
    "    for img in os.listdir(image_dir):\n",
    "        try: \n",
    "            img_peth=str(image_dir)+'/'+str(img)\n",
    "            image_array=jpg_image_to_array(img_peth)\n",
    "            label=label_img(img)\n",
    "            img_dataset.append([image_array])  \n",
    "#             label_dataset.append([label])\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "#     np.save('image_dataset.npy',img_dataset)\n",
    "#     return img_dataset\n",
    "    return img_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_list2_arr=np.array(image_list2)\n",
    "image_list2_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_list2_reshape=image_list2_arr.reshape(11755,100,100,3)\n",
    "image_list2_reshape.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_list2=create_label_list(new_image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn2.fit(X_train_test1_rescale, y_train_test1_1hot, validation_split=.2, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss, accuracy = cnn2.evaluate(X_test_test1_rescale, y_test_test1_1hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_label_list(image_dir):\n",
    "    img_dataset=[]\n",
    "    label_dataset=[]\n",
    "    for img in os.listdir(image_dir):\n",
    "        try: \n",
    "            img_peth=str(image_dir)+'/'+str(img)\n",
    "            image_array=jpg_image_to_array(img_peth)\n",
    "            label=label_img(img)\n",
    "#             img_dataset.append([image_array])  \n",
    "            label_dataset.append([label])\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "#     np.save('image_dataset.npy',img_dataset)\n",
    "#     return img_dataset\n",
    "    return label_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_list2_arr=np.array(label_list2)\n",
    "label_list2_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "aa = image_list2_arr\n",
    "bb = label_list2_arr\n",
    "\n",
    "cc = list(zip(aa, bb))\n",
    "\n",
    "random.shuffle(cc)\n",
    "\n",
    "aa1, bb1 = zip(*cc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# f2(label_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aa2=np.array(aa1)\n",
    "bb2=np.array(bb1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aa2=aa2.reshape(11755, 100, 100, 3)\n",
    "print(aa2.shape,bb2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "11758*0.8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train2=aa2[:9406]\n",
    "X_test2=aa2[9406:]\n",
    "y_train2=bb2[:9406]\n",
    "y_test2=bb2[9406:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train2_rescale=X_train2/255\n",
    "X_test2_rescale=X_test2/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X_train_test1=a2[:6533]\n",
    "# X_test_test1=a2[6533:]\n",
    "# y_train_test1=b2[:6533]\n",
    "# y_test_test1=b2[6533:]\n",
    "\n",
    "imgen_train = ImageDataGenerator(\n",
    "    rotation_range=8,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.15,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='constant'\n",
    ")\n",
    "\n",
    "imgen_test=ImageDataGenerator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_generator = imgen_train.flow(X_train_test1_rescale, y_train_test1_1hot, batch_size=64)\n",
    "test_generator = imgen_test.flow(X_test_test1_rescale, y_test_test1_1hot, batch_size=64)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn2.fit_generator(train_generator, steps_per_epoch=X_train_test1_rescale.shape[0] // 64, epochs=5, \n",
    "                    validation_data=test_generator, validation_steps=len(y_test_test1_1hot)//64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train2_1hot = encoder.fit_transform(y_train2)\n",
    "y_test2_1hot = encoder.fit_transform(y_test2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn3 = Sequential([\n",
    "    Conv2D(50, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=(100,100,3)),\n",
    "    MaxPool2D((2,2)),\n",
    "    \n",
    "    Conv2D(100, (3, 3), activation='relu'),\n",
    "#     BatchNormalization(axis=-1),\n",
    "    Dropout(.5),\n",
    "    MaxPool2D((2,2)),\n",
    "    \n",
    "#     Conv2D(100, (3, 3), activation='relu'),\n",
    "#     BatchNormalization(axis=-1),\n",
    "#     Dropout(.25),\n",
    "#     MaxPool2D((2,2)),\n",
    "    \n",
    "#     Conv2D(150, (3, 3), activation='relu'),\n",
    "#     BatchNormalization(axis=-1),\n",
    "#     Dropout(.25),\n",
    "#     MaxPool2D((2,2)),\n",
    "    \n",
    "#     Conv2D(100, (3, 3), activation='relu'),\n",
    "#     Dropout(.25),\n",
    "#     MaxPool2D((2,2)),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "#     BatchNormalization(axis=-1),\n",
    "    Dropout(.5),\n",
    "    Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn3.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(lr=0.2),\n",
    "#               optimizer=Adam(lr=0.2),\n",
    "#               optimizer=sgd,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn3.fit(X_train2_rescale, y_train2_1hot, validation_split=.2, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss, accuracy3 = cnn3.evaluate(X_test2_rescale, y_test2_1hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "pickle.dump(cnn3, open('cnn3.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "loaded_model3 = pickle.load(open('cnn3.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn4 = Sequential([\n",
    "    Conv2D(30, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=(100,100,3)),\n",
    "    MaxPool2D((2,2)),\n",
    "    \n",
    "    Conv2D(40, (3, 3), activation='relu'),\n",
    "    BatchNormalization(axis=-1),\n",
    "    Dropout(.25),\n",
    "    MaxPool2D((2,2)),\n",
    "    \n",
    "    Conv2D(60, (3, 3), activation='relu'),\n",
    "    BatchNormalization(axis=-1),\n",
    "    Dropout(.25),\n",
    "    MaxPool2D((2,2)),\n",
    "    \n",
    "    Conv2D(80, (3, 3), activation='relu'),\n",
    "    BatchNormalization(axis=-1),\n",
    "    Dropout(.25),\n",
    "    MaxPool2D((2,2)),\n",
    "    \n",
    "    Conv2D(100, (3, 3), activation='relu'),\n",
    "    Dropout(.25),\n",
    "    MaxPool2D((2,2)),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(axis=-1),\n",
    "    Dropout(.5),\n",
    "    Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn4.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(lr=0.2),\n",
    "#               optimizer=Adam(lr=0.2),\n",
    "#               optimizer=sgd,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imgen_train2 = ImageDataGenerator(\n",
    "    rotation_range=90,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.15,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    ")\n",
    "\n",
    "imgen_test2=ImageDataGenerator()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_generator2 = imgen_train2.flow(X_train2_rescale, y_train2_1hot, batch_size=64)\n",
    "test_generator2 = imgen_test2.flow(X_test2_rescale, y_test2_1hot, batch_size=64)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn4.fit_generator(train_generator2, steps_per_epoch=X_train2_rescale.shape[0] // 64, epochs=10, \n",
    "                    validation_data=test_generator2, validation_steps=len(y_test2_1hot)//64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "pickle.dump(cnn4, open('cnn4.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "loaded_model4 = pickle.load(open('cnn4.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
